#!/usr/bin/env python3
"""
å‘¨æŠ¥è®ºæ–‡ç›‘æ§è„šæœ¬ - æœç´¢ç ”ç©¶è€…æœ€è¿‘ä¸€å‘¨å‘å¸ƒçš„è®ºæ–‡
"""

import httpx
import csv
import io
from typing import List, Dict, Any
from datetime import datetime, timedelta
import re
from urllib.parse import urlencode


def fetch_researchers_from_tsv(url: str) -> List[Dict[str, Any]]:
    """
    ä» TSV URL è·å–ç ”ç©¶è€…æ•°æ®
    
    Args:
        url: Google Sheets TSV å¯¼å‡ºé“¾æ¥
        
    Returns:
        ç ”ç©¶è€…æ•°æ®åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨ httpx è·å– TSV æ•°æ®ï¼Œå…è®¸é‡å®šå‘
        with httpx.Client(follow_redirects=True) as client:
            response = client.get(url)
            response.raise_for_status()
            
        # è§£æ TSV æ•°æ®
        tsv_content = response.text
        print(f"è·å–åˆ°çš„åŸå§‹æ•°æ®:\n{tsv_content}\n")
        
        # ä½¿ç”¨ csv æ¨¡å—è§£æ TSV
        csv_reader = csv.reader(io.StringIO(tsv_content), delimiter='\t')
        
        # è¯»å–æ‰€æœ‰è¡Œ
        rows = list(csv_reader)
        
        if not rows:
            print("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®")
            return []
            
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜è¡Œï¼ˆé€šè¿‡æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦åŒ…å«æ˜æ˜¾çš„æ ‡é¢˜è¯æ±‡ï¼‰
        has_header = False
        if rows and len(rows) > 1:
            first_row = [cell.strip().lower() for cell in rows[0]]
            header_indicators = ['name', 'author', 'researcher', 'å§“å', 'ä½œè€…', 'ç ”ç©¶è€…']
            has_header = any(indicator in cell for cell in first_row for indicator in header_indicators)
        
        if has_header:
            headers = rows[0]
            data_rows = rows[1:]
            print(f"æ£€æµ‹åˆ°æ ‡é¢˜è¡Œ: {headers}")
        else:
            headers = []
            data_rows = rows
            print(f"æœªæ£€æµ‹åˆ°æ ‡é¢˜è¡Œï¼Œæ‰€æœ‰è¡Œéƒ½è§†ä¸ºæ•°æ®")
        
        print(f"æ•°æ®è¡Œæ•°: {len(data_rows)}")
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        researchers = []
        for i, row in enumerate(data_rows):
            if any(cell.strip() for cell in row):  # è·³è¿‡ç©ºè¡Œ
                if headers and len(headers) > 1:
                    # å¦‚æœæœ‰å¤šä¸ªåˆ—ï¼Œåˆ›å»ºå­—å…¸
                    researcher = {}
                    for j, header in enumerate(headers):
                        value = row[j] if j < len(row) else ""
                        researcher[header] = value.strip()
                    researchers.append(researcher)
                else:
                    # å¦‚æœåªæœ‰ä¸€åˆ—æˆ–æ²¡æœ‰æ ‡é¢˜ï¼Œå°†æ¯è¡Œä½œä¸ºç ”ç©¶è€…å§“å
                    name = " ".join(cell.strip() for cell in row if cell.strip())
                    if name:
                        researchers.append({"name": name, "row_index": i})
                        
        return researchers
        
    except httpx.RequestError as e:
        print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return []
    except Exception as e:
        print(f"è§£ææ•°æ®æ—¶å‡ºé”™: {e}")
        return []


def build_arxiv_search_url(author_name: str, date_from: str, date_to: str) -> str:
    """
    æ„å»º arXiv é«˜çº§æœç´¢ URL
    
    Args:
        author_name: ä½œè€…å§“å
        date_from: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        date_to: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        
    Returns:
        arXiv æœç´¢ URL
    """
    base_url = "https://arxiv.org/search/advanced"
    
    params = {
        'advanced': '',
        'terms-0-operator': 'AND',
        'terms-0-term': f'"{author_name}"',
        'terms-0-field': 'author',
        'classification-computer_science': 'y',
        'classification-physics_archives': 'all',
        'classification-include_cross_list': 'include',
        'date-year': '',
        'date-filter_by': 'date_range',
        'date-from_date': date_from,
        'date-to_date': date_to,
        'date-date_type': 'submitted_date',
        'abstracts': 'show',
        'size': '50',
        'order': '-announced_date_first'
    }
    
    return f"{base_url}?{urlencode(params)}"


def parse_arxiv_search_results(html_content: str) -> List[Dict[str, Any]]:
    """
    è§£æ arXiv æœç´¢ç»“æœé¡µé¢
    
    Args:
        html_content: HTML å†…å®¹
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
    """
    papers = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
    if "Sorry, your query returned no results" in html_content:
        return papers
    
    # å°è¯•å¤šç§è§£ææ¨¡å¼
    # æ¨¡å¼1: å¯»æ‰¾è®ºæ–‡æ¡ç›®
    paper_pattern = r'<li class="arxiv-result">.*?</li>'
    paper_matches = re.findall(paper_pattern, html_content, re.DOTALL)
    
    if not paper_matches:
        # æ¨¡å¼2: å¯»æ‰¾å…¶ä»–å¯èƒ½çš„è®ºæ–‡å®¹å™¨
        paper_pattern = r'<ol class="breathe-horizontal">.*?</ol>'
        section_matches = re.findall(paper_pattern, html_content, re.DOTALL)
        for section in section_matches:
            paper_pattern = r'<li>.*?</li>'
            paper_matches = re.findall(paper_pattern, section, re.DOTALL)
    
    for match in paper_matches:
        paper = {}
        
        # æå–æ ‡é¢˜ - å°è¯•å¤šç§æ¨¡å¼
        title_patterns = [
            r'<p class="title is-5 mathjax">\s*<a[^>]*>(.*?)</a>',
            r'<span class="title"[^>]*>(.*?)</span>',
            r'<div class="list-title[^>]*>\s*<a[^>]*>(.*?)</a>',
            r'<a[^>]*href="/abs/[^"]+[^>]*>(.*?)</a>',
        ]
        
        for pattern in title_patterns:
            title_match = re.search(pattern, match, re.DOTALL)
            if title_match:
                paper['title'] = re.sub(r'<[^>]+>', '', title_match.group(1)).strip()
                break
        
        # æå–arXiv ID - å°è¯•å¤šç§æ¨¡å¼
        id_patterns = [
            r'<a[^>]*href="/abs/([^"]+)"',
            r'arXiv:(\d{4}\.\d{4,5})',
            r'/abs/(\d{4}\.\d{4,5})',
        ]
        
        for pattern in id_patterns:
            id_match = re.search(pattern, match)
            if id_match:
                paper['arxiv_id'] = id_match.group(1)
                paper['url'] = f"https://arxiv.org/abs/{id_match.group(1)}"
                break
        
        # æå–ä½œè€… - å°è¯•å¤šç§æ¨¡å¼
        authors_patterns = [
            r'<p class="authors">.*?<a[^>]*>(.*?)</a>',
            r'<span class="descriptor">Authors:</span>\s*(.*?)(?:<span class="descriptor">|$)',
            r'Authors:\s*(.*?)(?:\n|<)',
        ]
        
        for pattern in authors_patterns:
            authors_matches = re.findall(pattern, match, re.DOTALL)
            if authors_matches:
                if pattern == authors_patterns[0]:  # ç¬¬ä¸€ç§æ¨¡å¼è¿”å›å¤šä¸ªåŒ¹é…
                    paper['authors'] = [re.sub(r'<[^>]+>', '', author).strip() for author in authors_matches]
                else:  # å…¶ä»–æ¨¡å¼è¿”å›å•ä¸ªå­—ç¬¦ä¸²ï¼Œéœ€è¦åˆ†å‰²
                    authors_text = re.sub(r'<[^>]+>', '', authors_matches[0]).strip()
                    paper['authors'] = [author.strip() for author in authors_text.split(',')]
                break
        
        # æå–æ‘˜è¦ - å°è¯•å¤šç§æ¨¡å¼
        abstract_patterns = [
            r'<span class="abstract-full has-text-grey-dark mathjax"[^>]*>(.*?)</span>',
            r'<span class="abstract-full"[^>]*>(.*?)</span>',
            r'<p class="abstract mathjax">(.*?)</p>',
            r'<blockquote class="abstract mathjax">(.*?)</blockquote>',
        ]
        
        for pattern in abstract_patterns:
            abstract_match = re.search(pattern, match, re.DOTALL)
            if abstract_match:
                paper['abstract'] = re.sub(r'<[^>]+>', '', abstract_match.group(1)).strip()
                break
        
        # æå–æäº¤æ—¥æœŸ
        date_patterns = [
            r'Submitted (\d{1,2} \w+ \d{4})',
            r'(\d{1,2} \w+ \d{4})',
        ]
        
        for pattern in date_patterns:
            date_match = re.search(pattern, match)
            if date_match:
                paper['submitted_date'] = date_match.group(1)
                break
        
        # å¦‚æœæå–åˆ°äº†è‡³å°‘ä¸€äº›ä¿¡æ¯å°±æ·»åŠ åˆ°åˆ—è¡¨
        if any(paper.values()):
            papers.append(paper)
    
    return papers


def fetch_papers_for_researcher(author_name: str, date_from: str, date_to: str) -> List[Dict[str, Any]]:
    """
    è·å–ç‰¹å®šç ”ç©¶è€…åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„è®ºæ–‡
    
    Args:
        author_name: ç ”ç©¶è€…å§“å
        date_from: å¼€å§‹æ—¥æœŸ
        date_to: ç»“æŸæ—¥æœŸ
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
    """
    try:
        # æ„å»ºæœç´¢URL
        search_url = build_arxiv_search_url(author_name, date_from, date_to)
        print(f"æœç´¢ {author_name} çš„è®ºæ–‡: {search_url}")
        
        # è·å–æœç´¢ç»“æœé¡µé¢
        with httpx.Client(follow_redirects=True, timeout=30.0) as client:
            response = client.get(search_url)
            response.raise_for_status()
        
        # è§£ææœç´¢ç»“æœ
        papers = parse_arxiv_search_results(response.text)
        
        # ä¸ºæ¯ç¯‡è®ºæ–‡æ·»åŠ æŸ¥è¯¢çš„ä½œè€…ä¿¡æ¯
        for paper in papers:
            paper['queried_author'] = author_name
            
        return papers
        
    except Exception as e:
        print(f"è·å– {author_name} çš„è®ºæ–‡æ—¶å‡ºé”™: {e}")
        return []


def get_weekly_papers_for_all_researchers(researchers: List[Dict[str, Any]], days: int = 7) -> Dict[str, List[Dict[str, Any]]]:
    """
    è·å–æ‰€æœ‰ç ”ç©¶è€…æœ€è¿‘ä¸€å‘¨å‘å¸ƒçš„è®ºæ–‡
    
    Args:
        researchers: ç ”ç©¶è€…åˆ—è¡¨
        days: æœç´¢æœ€è¿‘å‡ å¤©
        
    Returns:
        æŒ‰ç ”ç©¶è€…åˆ†ç»„çš„è®ºæ–‡å­—å…¸
    """
    # è·å–æ—¥æœŸèŒƒå›´
    today = datetime.now()
    start_date = today - timedelta(days=days)
    end_date = today
    
    start_date_str = start_date.strftime("%Y-%m-%d")
    end_date_str = end_date.strftime("%Y-%m-%d")
    
    print(f"\nğŸ“š æ­£åœ¨æœç´¢ {start_date_str} åˆ° {end_date_str} æœŸé—´å‘å¸ƒçš„è®ºæ–‡...")
    print("=" * 60)
    
    all_papers = {}
    
    for researcher in researchers:
        # è·å–ç ”ç©¶è€…å§“å
        if isinstance(researcher, dict):
            if "name" in researcher:
                author_name = researcher["name"]
            else:
                # å–ç¬¬ä¸€ä¸ªéç©ºå€¼ä½œä¸ºå§“å
                author_name = next((v for v in researcher.values() if v.strip()), "")
        else:
            author_name = str(researcher)
        
        if not author_name or author_name.lower() in ['aaa', 'test']:  # è·³è¿‡æµ‹è¯•æ•°æ®
            continue
            
        print(f"\næ­£åœ¨æœç´¢ {author_name} çš„è®ºæ–‡...")
        
        # è·å–è¯¥ç ”ç©¶è€…çš„è®ºæ–‡
        papers = fetch_papers_for_researcher(author_name, start_date_str, end_date_str)
        
        if papers:
            all_papers[author_name] = papers
            print(f"  âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        else:
            print(f"  âŒ æœªæ‰¾åˆ°è®ºæ–‡")
    
    return all_papers


def display_papers(all_papers: Dict[str, List[Dict[str, Any]]], period: str = "æœ€è¿‘ä¸€å‘¨") -> None:
    """
    æ˜¾ç¤ºæ‰€æœ‰è®ºæ–‡
    
    Args:
        all_papers: æŒ‰ç ”ç©¶è€…åˆ†ç»„çš„è®ºæ–‡å­—å…¸
        period: æ—¶é—´æ®µæè¿°
    """
    if not all_papers:
        print(f"\nğŸ“ {period}æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡")
        return
    
    total_papers = sum(len(papers) for papers in all_papers.values())
    print(f"\nğŸ‰ {period}å…±æ‰¾åˆ° {total_papers} ç¯‡è®ºæ–‡!")
    print("=" * 80)
    
    for author, papers in all_papers.items():
        print(f"\nğŸ‘¨â€ğŸ”¬ {author} ({len(papers)} ç¯‡è®ºæ–‡):")
        print("-" * 40)
        
        for i, paper in enumerate(papers, 1):
            print(f"\n{i}. ğŸ“„ {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
            
            if 'arxiv_id' in paper:
                print(f"   ğŸ”— arXiv ID: {paper['arxiv_id']}")
                print(f"   ğŸŒ é“¾æ¥: {paper.get('url', '')}")
            
            if 'authors' in paper and paper['authors']:
                authors_str = ", ".join(paper['authors'][:3])  # åªæ˜¾ç¤ºå‰3ä¸ªä½œè€…
                if len(paper['authors']) > 3:
                    authors_str += f" (ç­‰ {len(paper['authors'])} ä½ä½œè€…)"
                print(f"   ğŸ‘¥ ä½œè€…: {authors_str}")
            
            if 'submitted_date' in paper:
                print(f"   ğŸ“… æäº¤æ—¥æœŸ: {paper['submitted_date']}")
            
            if 'abstract' in paper and paper['abstract']:
                abstract = paper['abstract'][:200] + "..." if len(paper['abstract']) > 200 else paper['abstract']
                print(f"   ğŸ“ æ‘˜è¦: {abstract}")


def display_researchers(researchers: List[Dict[str, Any]]) -> None:
    """
    æ˜¾ç¤ºç ”ç©¶è€…åˆ—è¡¨
    
    Args:
        researchers: ç ”ç©¶è€…æ•°æ®åˆ—è¡¨
    """
    if not researchers:
        print("æ²¡æœ‰æ‰¾åˆ°ç ”ç©¶è€…æ•°æ®")
        return
        
    print(f"\næ‰¾åˆ° {len(researchers)} ä¸ªç ”ç©¶è€…:")
    print("=" * 50)
    
    for i, researcher in enumerate(researchers, 1):
        print(f"{i}. ", end="")
        
        if isinstance(researcher, dict):
            if "name" in researcher:
                print(f"å§“å: {researcher['name']}")
                # æ˜¾ç¤ºå…¶ä»–å­—æ®µ
                for key, value in researcher.items():
                    if key != "name" and key != "row_index" and value:
                        print(f"   {key}: {value}")
            else:
                # æ˜¾ç¤ºæ‰€æœ‰å­—æ®µ
                for key, value in researcher.items():
                    if value:
                        print(f"{key}: {value}")
        else:
            print(researcher)
        print()


def main():
    """ä¸»å‡½æ•°"""
    # Google Sheets TSV å¯¼å‡ºé“¾æ¥
    tsv_url = "https://docs.google.com/spreadsheets/d/1itjnV2U-Eh0F1T0LIGuLjzIhgL9f_OD8tbkMUG-Onic/export?format=tsv&id=1itjnV2U-Eh0F1T0LIGuLjzIhgL9f_OD8tbkMUG-Onic&gid=0"
    
    print("ğŸ“š å‘¨æŠ¥è®ºæ–‡ç›‘æ§ - è·å–ç ”ç©¶è€…æœ€è¿‘ä¸€å‘¨å‘å¸ƒçš„è®ºæ–‡")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"URL: {tsv_url}\n")
    
    # è·å–ç ”ç©¶è€…æ•°æ®
    researchers = fetch_researchers_from_tsv(tsv_url)
    
    # æ˜¾ç¤ºç ”ç©¶è€…åˆ—è¡¨
    display_researchers(researchers)
    
    if researchers:
        # è·å–æ‰€æœ‰ç ”ç©¶è€…æœ€è¿‘ä¸€å‘¨å‘å¸ƒçš„è®ºæ–‡
        all_papers = get_weekly_papers_for_all_researchers(researchers, days=7)
        
        # æ˜¾ç¤ºè®ºæ–‡ç»“æœ
        display_papers(all_papers, "æœ€è¿‘ä¸€å‘¨")
        
        return researchers, all_papers
    else:
        return [], {}


if __name__ == "__main__":
    researchers_data, papers_data = main() 