#!/usr/bin/env python3
"""
è®ºæ–‡å†…å®¹é‡‡é›†æ¨¡å— - ä»arXivè·å–è®ºæ–‡å®Œæ•´å†…å®¹
æ”¯æŒå¤šç§å†…å®¹è·å–æ–¹å¼å’Œæ™ºèƒ½å†…å®¹æå–
"""

import httpx
import re
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
import time
import json
from urllib.parse import urljoin

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PaperCollector:
    """è®ºæ–‡å†…å®¹é‡‡é›†å™¨"""
    
    def __init__(self):
        self.session = httpx.Client(
            timeout=30.0,
            headers={
                "User-Agent": "ArXiv-Follow-Collector/1.0 (Academic Research Tool)"
            },
            follow_redirects=True
        )
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'session'):
            self.session.close()
    
    def get_paper_abstract_page(self, arxiv_id: str) -> Optional[str]:
        """
        è·å–è®ºæ–‡æ‘˜è¦é¡µé¢å†…å®¹
        
        Args:
            arxiv_id: arXivè®ºæ–‡ID (å¦‚: 2501.12345)
            
        Returns:
            æ‘˜è¦é¡µé¢HTMLå†…å®¹
        """
        try:
            url = f"https://arxiv.org/abs/{arxiv_id}"
            logger.info(f"è·å–è®ºæ–‡æ‘˜è¦é¡µé¢: {url}")
            
            response = self.session.get(url)
            response.raise_for_status()
            
            return response.text
            
        except Exception as e:
            logger.error(f"è·å–æ‘˜è¦é¡µé¢å¤±è´¥ {arxiv_id}: {e}")
            return None
    
    def extract_paper_metadata(self, html_content: str, arxiv_id: str) -> Dict[str, Any]:
        """
        ä»æ‘˜è¦é¡µé¢æå–è¯¦ç»†å…ƒæ•°æ®
        
        Args:
            html_content: HTMLå†…å®¹
            arxiv_id: arXiv ID
            
        Returns:
            è¯¦ç»†çš„è®ºæ–‡å…ƒæ•°æ®
        """
        metadata = {
            'arxiv_id': arxiv_id,
            'url': f"https://arxiv.org/abs/{arxiv_id}",
            'pdf_url': f"https://arxiv.org/pdf/{arxiv_id}.pdf",
            'collection_time': datetime.now().isoformat()
        }
        
        try:
            # æå–æ ‡é¢˜
            title_pattern = r'<h1 class="title mathjax"[^>]*>\s*<span[^>]*>\s*(.*?)\s*</span>\s*</h1>'
            title_match = re.search(title_pattern, html_content, re.DOTALL)
            if title_match:
                title = re.sub(r'<[^>]+>', '', title_match.group(1)).strip()
                metadata['title'] = title
            
            # æå–ä½œè€…ä¿¡æ¯ï¼ˆæ›´è¯¦ç»†ï¼‰
            authors_pattern = r'<div class="authors"[^>]*>(.*?)</div>'
            authors_match = re.search(authors_pattern, html_content, re.DOTALL)
            if authors_match:
                authors_html = authors_match.group(1)
                # æå–ä½œè€…é“¾æ¥å’Œå§“å
                author_links = re.findall(r'<a[^>]+>([^<]+)</a>', authors_html)
                if author_links:
                    metadata['authors'] = [author.strip() for author in author_links]
                else:
                    # å¤‡é€‰æ–¹æ¡ˆï¼šæå–çº¯æ–‡æœ¬ä½œè€…
                    authors_text = re.sub(r'<[^>]+>', '', authors_html)
                    authors = [author.strip() for author in authors_text.split(',')]
                    metadata['authors'] = [author for author in authors if author]
            
            # æå–å®Œæ•´æ‘˜è¦
            abstract_pattern = r'<blockquote class="abstract mathjax"[^>]*>\s*<span[^>]*>Abstract:</span>\s*(.*?)\s*</blockquote>'
            abstract_match = re.search(abstract_pattern, html_content, re.DOTALL)
            if abstract_match:
                abstract = re.sub(r'<[^>]+>', '', abstract_match.group(1)).strip()
                abstract = re.sub(r'\s+', ' ', abstract)
                metadata['abstract'] = abstract
            
            # æå–å­¦ç§‘åˆ†ç±»
            subjects_pattern = r'<span class="primary-subject">([^<]+)</span>'
            subjects_match = re.search(subjects_pattern, html_content)
            if subjects_match:
                metadata['primary_subject'] = subjects_match.group(1).strip()
            
            # æå–æ‰€æœ‰åˆ†ç±»
            all_subjects = re.findall(r'<td class="tablecell subjects">([^<]+)</td>', html_content)
            if all_subjects:
                subjects = [subj.strip() for subj in all_subjects[0].split(';') if subj.strip()]
                metadata['subjects'] = subjects
            
            # æå–æäº¤æ—¥æœŸ
            submitted_pattern = r'<td class="tablecell"[^>]*>\[Submitted[^<]*on\s+([^\]]+)\]</td>'
            submitted_match = re.search(submitted_pattern, html_content)
            if submitted_match:
                metadata['submitted_date'] = submitted_match.group(1).strip()
            
            # æå–è¯„è®ºä¿¡æ¯
            comments_pattern = r'<td class="tablecell comments mathjax">([^<]+)</td>'
            comments_match = re.search(comments_pattern, html_content)
            if comments_match:
                metadata['comments'] = comments_match.group(1).strip()
            
            # æå–æœŸåˆŠå¼•ç”¨ä¿¡æ¯
            journal_pattern = r'<td class="tablecell jref">([^<]+)</td>'
            journal_match = re.search(journal_pattern, html_content)
            if journal_match:
                metadata['journal_ref'] = journal_match.group(1).strip()
            
            # æå–DOI
            doi_pattern = r'<td class="tablecell doi"[^>]*><a[^>]+>([^<]+)</a></td>'
            doi_match = re.search(doi_pattern, html_content)
            if doi_match:
                metadata['doi'] = doi_match.group(1).strip()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰HTMLç‰ˆæœ¬
            html_version_pattern = r'<a[^>]+href="([^"]*html[^"]*)"[^>]*>HTML</a>'
            html_version_match = re.search(html_version_pattern, html_content)
            if html_version_match:
                metadata['html_url'] = urljoin("https://arxiv.org", html_version_match.group(1))
            
        except Exception as e:
            logger.error(f"æå–å…ƒæ•°æ®æ—¶å‡ºé”™: {e}")
        
        return metadata
    
    def get_paper_html_content(self, arxiv_id: str) -> Optional[str]:
        """
        å°è¯•è·å–è®ºæ–‡çš„HTMLç‰ˆæœ¬å†…å®¹
        
        Args:
            arxiv_id: arXivè®ºæ–‡ID
            
        Returns:
            HTMLæ ¼å¼çš„è®ºæ–‡å†…å®¹ï¼Œå¦‚æœä¸å¯ç”¨åˆ™è¿”å›None
        """
        try:
            # æ£€æŸ¥HTMLç‰ˆæœ¬æ˜¯å¦å¯ç”¨
            html_url = f"https://arxiv.org/html/{arxiv_id}"
            logger.info(f"å°è¯•è·å–HTMLç‰ˆæœ¬: {html_url}")
            
            response = self.session.get(html_url)
            
            if response.status_code == 200:
                logger.info(f"æˆåŠŸè·å–HTMLç‰ˆæœ¬: {arxiv_id}")
                return response.text
            else:
                logger.info(f"HTMLç‰ˆæœ¬ä¸å¯ç”¨: {arxiv_id} (çŠ¶æ€ç : {response.status_code})")
                return None
                
        except Exception as e:
            logger.warning(f"è·å–HTMLç‰ˆæœ¬å¤±è´¥ {arxiv_id}: {e}")
            return None
    
    def extract_text_from_html(self, html_content: str) -> Dict[str, Any]:
        """
        ä»HTMLå†…å®¹ä¸­æå–ç»“æ„åŒ–æ–‡æœ¬
        
        Args:
            html_content: HTMLå†…å®¹
            
        Returns:
            æå–çš„ç»“æ„åŒ–æ–‡æœ¬ä¿¡æ¯
        """
        extracted = {
            'has_html_version': True,
            'extraction_time': datetime.now().isoformat()
        }
        
        try:
            # æå–æ ‡é¢˜
            title_patterns = [
                r'<h1[^>]*class="[^"]*title[^"]*"[^>]*>([^<]+)</h1>',
                r'<title>([^<]+)</title>',
                r'<h1[^>]*>([^<]+)</h1>'
            ]
            
            for pattern in title_patterns:
                title_match = re.search(pattern, html_content, re.IGNORECASE)
                if title_match:
                    extracted['title'] = title_match.group(1).strip()
                    break
            
            # æå–ç« èŠ‚å†…å®¹
            sections = []
            
            # æŸ¥æ‰¾æ‰€æœ‰æ ‡é¢˜å’Œå†…å®¹
            section_pattern = r'<h([1-6])[^>]*>([^<]+)</h\1>(.*?)(?=<h[1-6]|$)'
            section_matches = re.findall(section_pattern, html_content, re.DOTALL | re.IGNORECASE)
            
            for level, title, content in section_matches:
                # æ¸…ç†å†…å®¹
                clean_content = re.sub(r'<[^>]+>', ' ', content)
                clean_content = re.sub(r'\s+', ' ', clean_content).strip()
                
                if clean_content and len(clean_content) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                    sections.append({
                        'level': int(level),
                        'title': title.strip(),
                        'content': clean_content[:2000]  # é™åˆ¶é•¿åº¦
                    })
            
            if sections:
                extracted['sections'] = sections
            
            # æå–æ‘˜è¦
            abstract_patterns = [
                r'<div[^>]*class="[^"]*abstract[^"]*"[^>]*>(.*?)</div>',
                r'<section[^>]*class="[^"]*abstract[^"]*"[^>]*>(.*?)</section>',
                r'<p[^>]*class="[^"]*abstract[^"]*"[^>]*>(.*?)</p>'
            ]
            
            for pattern in abstract_patterns:
                abstract_match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
                if abstract_match:
                    abstract = re.sub(r'<[^>]+>', ' ', abstract_match.group(1))
                    abstract = re.sub(r'\s+', ' ', abstract).strip()
                    if len(abstract) > 50:
                        extracted['html_abstract'] = abstract
                        break
            
            # æå–å‚è€ƒæ–‡çŒ®æ•°é‡
            ref_patterns = [
                r'<div[^>]*class="[^"]*reference[^"]*"',
                r'<li[^>]*class="[^"]*reference[^"]*"',
                r'\[(\d+)\].*?</li>'
            ]
            
            ref_count = 0
            for pattern in ref_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                ref_count = max(ref_count, len(matches))
            
            if ref_count > 0:
                extracted['reference_count'] = ref_count
            
            # ä¼°ç®—æ–‡æœ¬é•¿åº¦
            all_text = re.sub(r'<[^>]+>', ' ', html_content)
            all_text = re.sub(r'\s+', ' ', all_text)
            extracted['estimated_word_count'] = len(all_text.split())
            
        except Exception as e:
            logger.error(f"ä»HTMLæå–æ–‡æœ¬æ—¶å‡ºé”™: {e}")
        
        return extracted
    
    def collect_paper_content(self, arxiv_id: str) -> Dict[str, Any]:
        """
        é‡‡é›†è®ºæ–‡å®Œæ•´å†…å®¹
        
        Args:
            arxiv_id: arXivè®ºæ–‡ID
            
        Returns:
            å®Œæ•´çš„è®ºæ–‡å†…å®¹ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹é‡‡é›†è®ºæ–‡å†…å®¹: {arxiv_id}")
        
        # è·å–åŸºç¡€å…ƒæ•°æ®
        abstract_html = self.get_paper_abstract_page(arxiv_id)
        if not abstract_html:
            return {'error': f'æ— æ³•è·å–è®ºæ–‡ {arxiv_id} çš„æ‘˜è¦é¡µé¢'}
        
        # æå–å…ƒæ•°æ®
        metadata = self.extract_paper_metadata(abstract_html, arxiv_id)
        
        # å°è¯•è·å–HTMLç‰ˆæœ¬
        html_content = self.get_paper_html_content(arxiv_id)
        
        if html_content:
            # æå–HTMLç‰ˆæœ¬çš„è¯¦ç»†å†…å®¹
            html_extracted = self.extract_text_from_html(html_content)
            metadata.update(html_extracted)
        else:
            metadata['has_html_version'] = False
        
        # æ·»åŠ é‡‡é›†ç»Ÿè®¡
        metadata['content_sources'] = []
        if abstract_html:
            metadata['content_sources'].append('abstract_page')
        if html_content:
            metadata['content_sources'].append('html_version')
        
        logger.info(f"è®ºæ–‡å†…å®¹é‡‡é›†å®Œæˆ: {arxiv_id}, æ•°æ®æº: {metadata.get('content_sources', [])}")
        
        return metadata
    
    def collect_multiple_papers(self, arxiv_ids: List[str], delay: float = 1.0) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡é‡‡é›†å¤šç¯‡è®ºæ–‡å†…å®¹
        
        Args:
            arxiv_ids: arXiv IDåˆ—è¡¨
            delay: è¯·æ±‚é—´éš”å»¶è¿Ÿ(ç§’)
            
        Returns:
            è®ºæ–‡IDåˆ°å†…å®¹çš„æ˜ å°„
        """
        results = {}
        
        logger.info(f"å¼€å§‹æ‰¹é‡é‡‡é›† {len(arxiv_ids)} ç¯‡è®ºæ–‡")
        
        for i, arxiv_id in enumerate(arxiv_ids):
            try:
                results[arxiv_id] = self.collect_paper_content(arxiv_id)
                
                # è¿›åº¦æ˜¾ç¤º
                if i % 5 == 0 or i == len(arxiv_ids) - 1:
                    logger.info(f"é‡‡é›†è¿›åº¦: {i + 1}/{len(arxiv_ids)}")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                if i < len(arxiv_ids) - 1:
                    time.sleep(delay)
                    
            except Exception as e:
                logger.error(f"é‡‡é›†è®ºæ–‡ {arxiv_id} æ—¶å‡ºé”™: {e}")
                results[arxiv_id] = {'error': str(e)}
        
        logger.info(f"æ‰¹é‡é‡‡é›†å®Œæˆï¼ŒæˆåŠŸ: {len([r for r in results.values() if 'error' not in r])}/{len(arxiv_ids)}")
        
        return results


def collect_paper_content(arxiv_id: str) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šé‡‡é›†å•ç¯‡è®ºæ–‡å†…å®¹
    
    Args:
        arxiv_id: arXivè®ºæ–‡ID
        
    Returns:
        è®ºæ–‡å†…å®¹ä¿¡æ¯
    """
    collector = PaperCollector()
    try:
        return collector.collect_paper_content(arxiv_id)
    finally:
        collector.session.close()


def collect_multiple_papers(arxiv_ids: List[str], delay: float = 1.0) -> Dict[str, Dict[str, Any]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ‰¹é‡é‡‡é›†è®ºæ–‡å†…å®¹
    
    Args:
        arxiv_ids: arXiv IDåˆ—è¡¨
        delay: è¯·æ±‚é—´éš”å»¶è¿Ÿ(ç§’)
        
    Returns:
        è®ºæ–‡IDåˆ°å†…å®¹çš„æ˜ å°„
    """
    collector = PaperCollector()
    try:
        return collector.collect_multiple_papers(arxiv_ids, delay)
    finally:
        collector.session.close()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_arxiv_id = "2501.12345"  # ç¤ºä¾‹IDï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢
    
    print(f"ğŸ§ª æµ‹è¯•è®ºæ–‡å†…å®¹é‡‡é›†: {test_arxiv_id}")
    
    try:
        result = collect_paper_content(test_arxiv_id)
        print("\nğŸ“„ é‡‡é›†ç»“æœ:")
        print(f"æ ‡é¢˜: {result.get('title', 'N/A')}")
        print(f"ä½œè€…: {result.get('authors', 'N/A')}")
        print(f"æ‘˜è¦é•¿åº¦: {len(result.get('abstract', ''))}")
        print(f"HTMLç‰ˆæœ¬: {'æ˜¯' if result.get('has_html_version') else 'å¦'}")
        print(f"å†…å®¹æº: {result.get('content_sources', [])}")
        
        if result.get('sections'):
            print(f"å‘ç°ç« èŠ‚æ•°: {len(result['sections'])}")
            for section in result['sections'][:3]:  # æ˜¾ç¤ºå‰3ä¸ªç« èŠ‚
                print(f"  - {section['title']} (çº§åˆ« {section['level']})")
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}") 