#!/usr/bin/env python3
"""
ç°ä»£åŒ–è®ºæ–‡ç›‘æ§æ¨¡å—

æä¾›æ™ºèƒ½çš„è®ºæ–‡ç›‘æ§ã€åˆ†æå’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½ã€‚
"""

import logging
from datetime import datetime, timedelta
from typing import Any

from ..models import (
    SearchFilters,
    SearchQuery,
    SearchResult,
    SearchType,
    Task,
    TaskStatus,
    TaskType,
)
from ..models.config import AppConfig
from .analyzer import PaperAnalyzer
from .collector import ArxivCollector
from .engine import SearchEngine

logger = logging.getLogger(__name__)


class PaperMonitor:
    """ç°ä»£åŒ–è®ºæ–‡ç›‘æ§å™¨"""

    def __init__(self, config: AppConfig):
        """åˆå§‹åŒ–ç›‘æ§å™¨"""
        self.config = config
        self.collector = ArxivCollector(config)
        self.analyzer = (
            PaperAnalyzer(config) if config.is_feature_enabled("ai_analysis") else None
        )
        self.engine = SearchEngine(config)

        logger.info("è®ºæ–‡ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"AIåˆ†æ: {'å¯ç”¨' if self.analyzer else 'ç¦ç”¨'}")

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.collector.__aenter__()
        await self.engine.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.collector.__aexit__(exc_type, exc_val, exc_tb)
        await self.engine.__aexit__(exc_type, exc_val, exc_tb)

    async def monitor_researchers(
        self, researchers: list[str], days_back: int = 1
    ) -> SearchResult:
        """ç›‘æ§ç ”ç©¶è€…çš„æ–°è®ºæ–‡"""
        query = SearchQuery(
            query_id=f"researchers_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            search_type=SearchType.RESEARCHER,
            query_text=f"ç›‘æ§ {len(researchers)} ä½ç ”ç©¶è€…",
            researchers=researchers,
            filters=SearchFilters(days_back=days_back, max_results=100),
        )

        result = await self.engine.search(query)

        if result.success and self.analyzer:
            # å¯¹ç»“æœè¿›è¡ŒAIåˆ†æ
            analyzed_papers = []
            for paper_data in result.papers:
                try:
                    analysis = await self.analyzer.analyze_paper_significance(
                        paper_data
                    )
                    paper_data["ai_analysis"] = analysis
                    analyzed_papers.append(paper_data)
                except Exception as e:
                    logger.warning(f"åˆ†æè®ºæ–‡å¤±è´¥: {e}")
                    analyzed_papers.append(paper_data)

            result.papers = analyzed_papers

        return result

    async def monitor_topics(
        self, topics: list[str], days_back: int = 1
    ) -> SearchResult:
        """ç›‘æ§ä¸»é¢˜çš„æ–°è®ºæ–‡"""
        query = SearchQuery(
            query_id=f"topics_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            search_type=SearchType.TOPIC,
            query_text=f"ç›‘æ§ä¸»é¢˜: {', '.join(topics)}",
            topics=topics,
            filters=SearchFilters(days_back=days_back, max_results=100),
        )

        result = await self.engine.search(query)

        if result.success and self.analyzer:
            # æŒ‰é‡è¦æ€§æ’åº
            scored_papers = []
            for paper_data in result.papers:
                try:
                    analysis = await self.analyzer.analyze_paper_significance(
                        paper_data
                    )
                    paper_data["ai_analysis"] = analysis
                    paper_data["importance_score"] = analysis.get(
                        "importance_score", 5.0
                    )
                    scored_papers.append(paper_data)
                except Exception as e:
                    logger.warning(f"åˆ†æè®ºæ–‡å¤±è´¥: {e}")
                    paper_data["importance_score"] = 5.0
                    scored_papers.append(paper_data)

            # æŒ‰é‡è¦æ€§è¯„åˆ†æ’åº
            scored_papers.sort(
                key=lambda x: x.get("importance_score", 5.0), reverse=True
            )
            result.papers = scored_papers

        return result

    async def daily_monitor(
        self,
        researchers: list[str] | None = None,
        topics: list[str] | None = None,
    ) -> dict[str, Any]:
        """æ¯æ—¥ç›‘æ§"""
        results = {
            "timestamp": datetime.now().isoformat(),
            "date": datetime.now().strftime("%Y-%m-%d"),
            "researcher_results": None,
            "topic_results": None,
            "summary": {},
            "success": True,
        }

        try:
            # ç›‘æ§ç ”ç©¶è€…
            if researchers:
                logger.info(f"å¼€å§‹ç›‘æ§ {len(researchers)} ä½ç ”ç©¶è€…")
                results["researcher_results"] = await self.monitor_researchers(
                    researchers, days_back=1
                )

            # ç›‘æ§ä¸»é¢˜
            if topics:
                logger.info(f"å¼€å§‹ç›‘æ§ä¸»é¢˜: {', '.join(topics)}")
                results["topic_results"] = await self.monitor_topics(
                    topics, days_back=1
                )

            # ç”Ÿæˆæ‘˜è¦
            results["summary"] = self._generate_daily_summary(results)

            logger.info("æ¯æ—¥ç›‘æ§å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¯æ—¥ç›‘æ§å¤±è´¥: {e}")
            results["success"] = False
            results["error"] = str(e)

        return results

    async def weekly_monitor(
        self,
        researchers: list[str] | None = None,
        topics: list[str] | None = None,
    ) -> dict[str, Any]:
        """æ¯å‘¨ç›‘æ§"""
        results = {
            "timestamp": datetime.now().isoformat(),
            "week_start": (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d"),
            "week_end": datetime.now().strftime("%Y-%m-%d"),
            "researcher_results": None,
            "topic_results": None,
            "summary": {},
            "success": True,
        }

        try:
            # ç›‘æ§ç ”ç©¶è€…ï¼ˆè¿‡å»7å¤©ï¼‰
            if researchers:
                logger.info(f"å¼€å§‹æ¯å‘¨ç›‘æ§ {len(researchers)} ä½ç ”ç©¶è€…")
                results["researcher_results"] = await self.monitor_researchers(
                    researchers, days_back=7
                )

            # ç›‘æ§ä¸»é¢˜ï¼ˆè¿‡å»7å¤©ï¼‰
            if topics:
                logger.info(f"å¼€å§‹æ¯å‘¨ç›‘æ§ä¸»é¢˜: {', '.join(topics)}")
                results["topic_results"] = await self.monitor_topics(
                    topics, days_back=7
                )

            # ç”Ÿæˆæ‘˜è¦
            results["summary"] = self._generate_weekly_summary(results)

            logger.info("æ¯å‘¨ç›‘æ§å®Œæˆ")

        except Exception as e:
            logger.error(f"æ¯å‘¨ç›‘æ§å¤±è´¥: {e}")
            results["success"] = False
            results["error"] = str(e)

        return results

    def _generate_daily_summary(self, results: dict[str, Any]) -> dict[str, Any]:
        """ç”Ÿæˆæ¯æ—¥æ‘˜è¦"""
        summary = {
            "total_papers": 0,
            "researcher_papers": 0,
            "topic_papers": 0,
            "top_papers": [],
            "ai_insights": None,
        }

        # ç»Ÿè®¡ç ”ç©¶è€…è®ºæ–‡
        if results["researcher_results"] and results["researcher_results"].success:
            summary["researcher_papers"] = len(results["researcher_results"].papers)
            summary["total_papers"] += summary["researcher_papers"]

        # ç»Ÿè®¡ä¸»é¢˜è®ºæ–‡
        if results["topic_results"] and results["topic_results"].success:
            summary["topic_papers"] = len(results["topic_results"].papers)
            summary["total_papers"] += summary["topic_papers"]

        # æ”¶é›†æ‰€æœ‰è®ºæ–‡å¹¶æŒ‰é‡è¦æ€§æ’åº
        all_papers = []

        if results["researcher_results"] and results["researcher_results"].success:
            all_papers.extend(results["researcher_results"].papers)

        if results["topic_results"] and results["topic_results"].success:
            all_papers.extend(results["topic_results"].papers)

        # å»é‡å¹¶æŒ‰é‡è¦æ€§æ’åº
        unique_papers = {
            p.get("arxiv_id"): p for p in all_papers if p.get("arxiv_id")
        }.values()
        sorted_papers = sorted(
            unique_papers, key=lambda x: x.get("importance_score", 5.0), reverse=True
        )

        # é€‰å–å‰5ç¯‡è®ºæ–‡
        summary["top_papers"] = list(sorted_papers)[:5]

        # ç”ŸæˆAIæ´å¯Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.analyzer and summary["top_papers"]:
            try:
                summary["ai_insights"] = self._generate_ai_insights(
                    summary["top_papers"]
                )
            except Exception as e:
                logger.warning(f"ç”ŸæˆAIæ´å¯Ÿå¤±è´¥: {e}")

        return summary

    def _generate_weekly_summary(self, results: dict[str, Any]) -> dict[str, Any]:
        """ç”Ÿæˆæ¯å‘¨æ‘˜è¦"""
        summary = self._generate_daily_summary(results)  # å¤ç”¨æ¯æ—¥æ‘˜è¦é€»è¾‘

        # æ·»åŠ å‘¨æŠ¥ç‰¹æœ‰çš„ç»Ÿè®¡
        summary["weekly_trends"] = self._analyze_weekly_trends(results)

        return summary

    def _analyze_weekly_trends(self, results: dict[str, Any]) -> dict[str, Any]:
        """åˆ†ææ¯å‘¨è¶‹åŠ¿"""
        trends = {
            "hot_topics": [],
            "productive_researchers": [],
            "research_directions": [],
        }

        # åˆ†æçƒ­é—¨ä¸»é¢˜
        all_papers = []
        if results["topic_results"] and results["topic_results"].success:
            all_papers.extend(results["topic_results"].papers)

        if all_papers:
            # ç»Ÿè®¡åˆ†ç±»é¢‘æ¬¡
            category_counts = {}
            for paper in all_papers:
                categories = paper.get("categories", [])
                for cat in categories:
                    category_counts[cat] = category_counts.get(cat, 0) + 1

            # è·å–å‰5ä¸ªçƒ­é—¨åˆ†ç±»
            hot_categories = sorted(
                category_counts.items(), key=lambda x: x[1], reverse=True
            )[:5]
            trends["hot_topics"] = [
                {"category": cat, "count": count} for cat, count in hot_categories
            ]

        # åˆ†æé«˜äº§ç ”ç©¶è€…
        if results["researcher_results"] and results["researcher_results"].success:
            author_counts = {}
            for paper in results["researcher_results"].papers:
                authors = paper.get("authors", [])
                for author in authors:
                    author_counts[author] = author_counts.get(author, 0) + 1

            productive_authors = sorted(
                author_counts.items(), key=lambda x: x[1], reverse=True
            )[:5]
            trends["productive_researchers"] = [
                {"name": name, "papers": count} for name, count in productive_authors
            ]

        return trends

    def _generate_ai_insights(self, papers: list[dict[str, Any]]) -> dict[str, Any]:
        """ç”ŸæˆAIæ´å¯Ÿ"""
        insights = {
            "summary": "",
            "key_trends": [],
            "recommendations": [],
            "innovation_level": 0.0,
        }

        if not papers:
            return insights

        # åˆ†æåˆ›æ–°æ°´å¹³
        scores = [
            p.get("importance_score", 5.0) for p in papers if p.get("importance_score")
        ]
        if scores:
            insights["innovation_level"] = sum(scores) / len(scores)

        # æå–å…³é”®è¶‹åŠ¿
        all_categories = []
        for paper in papers:
            all_categories.extend(paper.get("categories", []))

        category_freq = {}
        for cat in all_categories:
            category_freq[cat] = category_freq.get(cat, 0) + 1

        insights["key_trends"] = [
            cat
            for cat, _ in sorted(
                category_freq.items(), key=lambda x: x[1], reverse=True
            )[:3]
        ]

        # ç”Ÿæˆæ‘˜è¦
        insights["summary"] = (
            f"å‘ç° {len(papers)} ç¯‡é«˜è´¨é‡è®ºæ–‡ï¼Œå¹³å‡é‡è¦æ€§è¯„åˆ† {insights['innovation_level']:.1f}/10"
        )

        # ç”Ÿæˆå»ºè®®
        if insights["innovation_level"] > 7.0:
            insights["recommendations"].append("å‘ç°å¤šç¯‡é«˜å½±å“åŠ›è®ºæ–‡ï¼Œå»ºè®®æ·±å…¥ç ”è¯»")
        if len(insights["key_trends"]) > 1:
            insights["recommendations"].append("æ³¨æ„è·¨é¢†åŸŸç ”ç©¶è¶‹åŠ¿")

        return insights

    async def create_monitoring_task(
        self, task_type: TaskType, parameters: dict[str, Any]
    ) -> Task:
        """åˆ›å»ºç›‘æ§ä»»åŠ¡"""
        from ..models import Task, TaskPriority

        task = Task(
            task_id=f"{task_type.value}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            task_type=task_type,
            title=f"è®ºæ–‡ç›‘æ§ä»»åŠ¡ - {task_type.value}",
            description=f"è‡ªåŠ¨ç›‘æ§ä»»åŠ¡ï¼Œå‚æ•°: {parameters}",
            status=TaskStatus.PENDING,
            priority=TaskPriority.NORMAL,
            parameters=parameters,
        )

        return task


def create_paper_monitor(config: AppConfig) -> PaperMonitor:
    """
    åˆ›å»ºè®ºæ–‡ç›‘æ§å™¨å®ä¾‹

    Args:
        config: åº”ç”¨ç¨‹åºé…ç½®

    Returns:
        è®ºæ–‡ç›‘æ§å™¨å®ä¾‹
    """
    return PaperMonitor(config)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•è®ºæ–‡ç›‘æ§åŠŸèƒ½")

    # ç¤ºä¾‹è®ºæ–‡æ•°æ®
    test_papers = [
        {
            "arxiv_id": "2501.12345",
            "title": "Deep Learning for Cybersecurity Applications",
            "authors": ["Zhang Wei", "Li Ming"],
            "abstract": "This paper presents novel deep learning approaches for cybersecurity...",
            "url": "https://arxiv.org/abs/2501.12345",
        }
    ]

    monitor = create_paper_monitor(AppConfig())

    print(f"AIåˆ†æ: {'å¯ç”¨' if monitor.analyzer else 'ç¦ç”¨'}")

    # æµ‹è¯•æ¯æ—¥ç›‘æ§
    result = monitor.daily_monitor(researchers=["Zhang Wei"], topics=["Deep Learning"])

    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œæ¯æ—¥ç›‘æ§ç»“æœ: {result}")

    # æµ‹è¯•æ¯å‘¨ç›‘æ§
    result = monitor.weekly_monitor(researchers=["Zhang Wei"], topics=["Deep Learning"])

    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œæ¯å‘¨ç›‘æ§ç»“æœ: {result}")

    # æµ‹è¯•ä»»åŠ¡åˆ›å»º
    task = monitor.create_monitoring_task(
        TaskType.DAILY_MONITOR,
        {"researchers": ["Zhang Wei"], "topics": ["Deep Learning"]},
    )

    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œä»»åŠ¡åˆ›å»º: {task}")
