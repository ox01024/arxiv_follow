#!/usr/bin/env python3
"""
åŸºäºä¸»é¢˜çš„è®ºæ–‡ç›‘æ§è„šæœ¬ - æœç´¢ç‰¹å®šä¸»é¢˜é¢†åŸŸçš„æœ€æ–°è®ºæ–‡
æ”¯æŒæ™ºèƒ½æ—¥æœŸå›é€€å’Œå¤šç§æœç´¢æ¨¡å¼
"""

import json
import os
from datetime import datetime, timedelta
from typing import Any
from urllib.parse import urlencode

import httpx

# å¯¼å…¥æ»´ç­”æ¸…å•é›†æˆå’Œé…ç½®
try:
    from ..config.settings import DIDA_API_CONFIG
    from ..integrations.dida import create_arxiv_task
    from ..services.researcher import parse_arxiv_search_results
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥é›†æˆæ¨¡å—ï¼Œç›¸å…³åŠŸèƒ½å°†è¢«ç¦ç”¨")

    def create_arxiv_task(*_args, **_kwargs):
        return {"success": False, "error": "æ¨¡å—æœªå¯¼å…¥"}

    def parse_arxiv_search_results(*_args, **_kwargs):
        return []

    DIDA_API_CONFIG = {"enable_bilingual": True}


def build_topic_search_url(
    topics: list[str],
    date_from: str | None = None,
    date_to: str | None = None,
    classification: str = "computer_science",
    field: str = "all",
    size: int = 50,
) -> str:
    """
    æ„å»ºåŸºäºä¸»é¢˜çš„ arXiv é«˜çº§æœç´¢ URL

    Args:
        topics: ä¸»é¢˜åˆ—è¡¨ (å¦‚ ["cs.AI", "cs.CR"])
        date_from: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD), None è¡¨ç¤ºä¸é™åˆ¶
        date_to: ç»“æŸæ—¥æœŸ (YYYY-MM-DD), None è¡¨ç¤ºä¸é™åˆ¶
        classification: åˆ†ç±»é¢†åŸŸ
        field: æœç´¢å­—æ®µ
        size: ç»“æœæ•°é‡

    Returns:
        arXiv æœç´¢ URL
    """
    base_url = "https://arxiv.org/search/advanced"

    params = {
        "advanced": "",
        "abstracts": "show",
        "size": str(size),
        "order": "-announced_date_first",
    }

    # æ·»åŠ ä¸»é¢˜æœç´¢æ¡ä»¶
    for i, topic in enumerate(topics):
        params[f"terms-{i}-operator"] = "AND"
        params[f"terms-{i}-term"] = topic
        params[f"terms-{i}-field"] = field

    # æ·»åŠ åˆ†ç±»
    if classification == "computer_science":
        params["classification-computer_science"] = "y"
    elif classification == "physics":
        params["classification-physics_archives"] = "all"

    params["classification-include_cross_list"] = "include"

    # æ·»åŠ æ—¥æœŸæ¡ä»¶
    if date_from and date_to:
        params["date-filter_by"] = "date_range"
        params["date-from_date"] = date_from
        params["date-to_date"] = date_to
        params["date-date_type"] = "submitted_date"
    else:
        params["date-filter_by"] = "all_dates"

    params["date-year"] = ""

    return f"{base_url}?{urlencode(params)}"





def fetch_papers_by_topic(
    topics: list[str],
    date_from: str | None = None,
    date_to: str | None = None,
    max_retries: int = 3,
) -> dict[str, Any]:
    """
    æ ¹æ®ä¸»é¢˜æœç´¢è®ºæ–‡ï¼Œæ”¯æŒæ™ºèƒ½æ—¥æœŸå›é€€

    Args:
        topics: ä¸»é¢˜åˆ—è¡¨
        date_from: å¼€å§‹æ—¥æœŸ
        date_to: ç»“æŸæ—¥æœŸ
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        åŒ…å«è®ºæ–‡åˆ—è¡¨å’Œæœç´¢ä¿¡æ¯çš„å­—å…¸
    """

    # å®šä¹‰æœç´¢ç­–ç•¥
    search_strategies = []

    if date_from and date_to:
        # ç­–ç•¥1: ç²¾ç¡®æ—¥æœŸèŒƒå›´
        search_strategies.append(
            {
                "name": f"ç²¾ç¡®æ—¥æœŸèŒƒå›´ ({date_from} åˆ° {date_to})",
                "date_from": date_from,
                "date_to": date_to,
            }
        )

        # ç­–ç•¥2: æ‰©å±•åˆ°æœ€è¿‘7å¤©
        try:
            end_date = datetime.strptime(date_to, "%Y-%m-%d")
            start_date = end_date - timedelta(days=7)
            search_strategies.append(
                {
                    "name": f'æœ€è¿‘7å¤© ({start_date.strftime("%Y-%m-%d")} åˆ° {date_to})',
                    "date_from": start_date.strftime("%Y-%m-%d"),
                    "date_to": date_to,
                }
            )
        except (ValueError, TypeError):
            pass

        # ç­–ç•¥3: æ‰©å±•åˆ°æœ€è¿‘30å¤©
        try:
            end_date = datetime.strptime(date_to, "%Y-%m-%d")
            start_date = end_date - timedelta(days=30)
            search_strategies.append(
                {
                    "name": f'æœ€è¿‘30å¤© ({start_date.strftime("%Y-%m-%d")} åˆ° {date_to})',
                    "date_from": start_date.strftime("%Y-%m-%d"),
                    "date_to": date_to,
                }
            )
        except (ValueError, TypeError):
            pass

    # ç­–ç•¥4: ä¸é™æ—¥æœŸ
    search_strategies.append({"name": "ä¸é™æ—¥æœŸ", "date_from": None, "date_to": None})

    results = {
        "topics": topics,
        "papers": [],
        "search_strategy_used": None,
        "total_results": 0,
        "search_url": None,
        "attempted_strategies": [],
    }

    # å°è¯•å„ç§æœç´¢ç­–ç•¥
    for strategy in search_strategies:
        try:
            print(f"ğŸ” å°è¯•æœç´¢ç­–ç•¥: {strategy['name']}")

            url = build_topic_search_url(
                topics=topics,
                date_from=strategy["date_from"],
                date_to=strategy["date_to"],
            )

            print(f"ğŸŒ æœç´¢URL: {url}")

            with httpx.Client(follow_redirects=True, timeout=30.0) as client:
                response = client.get(url)
                response.raise_for_status()

            papers = parse_arxiv_search_results(response.text)

            strategy_result = {
                "name": strategy["name"],
                "papers_found": len(papers),
                "total_available": papers[0].get("total_results", 0) if papers else 0,
                "url": url,
            }
            results["attempted_strategies"].append(strategy_result)

            print(f"ğŸ“Š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

            if papers:
                results["papers"] = papers
                results["search_strategy_used"] = strategy["name"]
                results["total_results"] = papers[0].get("total_results", len(papers))
                results["search_url"] = url
                break
            else:
                print("âŒ è¯¥ç­–ç•¥æœªæ‰¾åˆ°ç»“æœï¼Œå°è¯•ä¸‹ä¸€ä¸ªç­–ç•¥...")

        except Exception as e:
            print(f"âŒ æœç´¢ç­–ç•¥ '{strategy['name']}' å¤±è´¥: {e}")
            results["attempted_strategies"].append(
                {
                    "name": strategy["name"],
                    "error": str(e),
                    "url": url if "url" in locals() else None,
                }
            )
            continue

    return results


def display_search_results(results: dict[str, Any], limit: int = 10) -> None:
    """
    æ˜¾ç¤ºæœç´¢ç»“æœ

    Args:
        results: æœç´¢ç»“æœå­—å…¸
        limit: æ˜¾ç¤ºè®ºæ–‡æ•°é‡é™åˆ¶
    """
    print("\n" + "=" * 80)
    print("ğŸ” ä¸»é¢˜æœç´¢ç»“æœ")
    print(f"ğŸ·ï¸  æœç´¢ä¸»é¢˜: {' AND '.join(results['topics'])}")
    print(f"â° æœç´¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    # æ˜¾ç¤ºå°è¯•çš„æœç´¢ç­–ç•¥
    print("\nğŸ“‹ æœç´¢ç­–ç•¥å°è¯•è®°å½•:")
    for i, strategy in enumerate(results["attempted_strategies"], 1):
        if "error" in strategy:
            print(f"  {i}. âŒ {strategy['name']}: {strategy['error']}")
        else:
            print(
                f"  {i}. {'âœ…' if strategy['papers_found'] > 0 else 'âŒ'} {strategy['name']}: {strategy['papers_found']} ç¯‡è®ºæ–‡ (æ€»è®¡ {strategy['total_available']} ç¯‡)"
            )

    if not results["papers"]:
        print("\nâŒ æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½æœªæ‰¾åˆ°ç»“æœ")
        return

    print(f"\nğŸ¯ ä½¿ç”¨ç­–ç•¥: {results['search_strategy_used']}")
    print(
        f"ğŸ“Š æ˜¾ç¤ºå‰ {min(limit, len(results['papers']))} ç¯‡è®ºæ–‡ (æ€»è®¡ {results['total_results']} ç¯‡)"
    )
    print(f"ğŸ”— æœç´¢é“¾æ¥: {results['search_url']}")

    # æ˜¾ç¤ºè®ºæ–‡åˆ—è¡¨
    for i, paper in enumerate(results["papers"][:limit], 1):
        print(f"\n{'-'*60}")
        print(f"ğŸ“„ {i}. {paper.get('title', 'æ— æ ‡é¢˜')}")
        print(f"ğŸ†” arXiv ID: {paper['arxiv_id']}")
        print(f"ğŸ·ï¸  å­¦ç§‘åˆ†ç±»: {', '.join(paper.get('subjects', []))}")

        if paper.get("authors"):
            # æ˜¾ç¤ºæ‰€æœ‰ä½œè€…
            authors_display = ", ".join(paper["authors"])
            print(f"ğŸ‘¥ ä½œè€…: {authors_display}")

        if paper.get("submitted_date"):
            print(f"ğŸ“… æäº¤æ—¥æœŸ: {paper['submitted_date']}")

        print(f"ğŸŒ é“¾æ¥: {paper['url']}")

        if paper.get("abstract"):
            abstract = paper["abstract"]
            print(f"ğŸ“ æ‘˜è¦: {abstract}")

        # æ˜¾ç¤ºè¯„è®ºä¿¡æ¯
        if paper.get("comments"):
            print(f"ğŸ’¬ è¯„è®º: {paper['comments']}")

    if len(results["papers"]) > limit:
        print(
            f"\nğŸ’¡ è¿˜æœ‰ {len(results['papers']) - limit} ç¯‡è®ºæ–‡æœªæ˜¾ç¤ºï¼Œå¯è°ƒæ•´ limit å‚æ•°æŸ¥çœ‹æ›´å¤š"
        )


def get_topic_papers_with_smart_dates(
    topics: list[str], target_date: str | None = None, days_back: int = 1
) -> dict[str, Any]:
    """
    æ™ºèƒ½è·å–ä¸»é¢˜è®ºæ–‡ï¼Œæ”¯æŒæ—¥æœŸå›é€€

    Args:
        topics: ä¸»é¢˜åˆ—è¡¨
        target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼ŒNone è¡¨ç¤ºä»Šå¤©
        days_back: å›é€€å¤©æ•°

    Returns:
        æœç´¢ç»“æœå­—å…¸
    """
    if target_date is None:
        target_date = datetime.now().strftime("%Y-%m-%d")

    # è®¡ç®—æ—¥æœŸèŒƒå›´
    try:
        end_date = datetime.strptime(target_date, "%Y-%m-%d")
        start_date = end_date - timedelta(days=days_back - 1)
        date_from = start_date.strftime("%Y-%m-%d")
        date_to = end_date.strftime("%Y-%m-%d")
    except (ValueError, TypeError):
        date_from = None
        date_to = None

    return fetch_papers_by_topic(topics, date_from, date_to)


def create_topic_dida_task(
    topics: list[str], results: dict[str, Any], error: str = None
) -> None:
    """
    åˆ›å»ºä¸»é¢˜è®ºæ–‡æœç´¢çš„æ»´ç­”æ¸…å•ä»»åŠ¡

    Args:
        topics: æœç´¢ä¸»é¢˜åˆ—è¡¨
        results: æœç´¢ç»“æœå­—å…¸
        error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    """
    print("\nğŸ“ åˆ›å»ºæ»´ç­”æ¸…å•ä»»åŠ¡...")

    try:
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        papers = results.get("papers", []) if results else []
        paper_count = len(papers)

        # æ„å»ºä»»åŠ¡æ‘˜è¦ï¼ˆMarkdownæ ¼å¼ï¼‰
        topics_str = " AND ".join([f"`{topic}`" for topic in topics])
        " AND ".join(topics)
        if error:
            summary = f"âŒ **ä¸»é¢˜è®ºæ–‡æœç´¢æ‰§è¡Œå¤±è´¥**\n\n**ä¸»é¢˜:** {topics_str}\n**é”™è¯¯ä¿¡æ¯:** {error}"
            details = f"â° **æ‰§è¡Œæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        elif paper_count == 0:
            summary = f"ğŸ¯ **ä¸»é¢˜è®ºæ–‡æœç´¢æœªå‘ç°æ–°è®ºæ–‡**\n\n**ä¸»é¢˜:** {topics_str}"
            details = f"ğŸ” **æœç´¢ä¸»é¢˜:** {topics_str}\n"
            if results:
                details += "\n### ğŸ“‹ å°è¯•ç­–ç•¥\n"
                strategy_info = []
                for strategy in results.get("attempted_strategies", []):
                    if "error" in strategy:
                        strategy_info.append(
                            f"âŒ **{strategy['name']}:** {strategy['error']}"
                        )
                    else:
                        strategy_info.append(
                            f"âœ… **{strategy['name']}:** {strategy['papers_found']} ç¯‡"
                        )
                details += "\n".join(strategy_info)
            details += (
                f"\n\nâ° **æ‰§è¡Œæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            )
        else:
            summary = f"ğŸ‰ **ä¸»é¢˜è®ºæ–‡æœç´¢å‘ç° {paper_count} ç¯‡è®ºæ–‡ï¼**\n\n**ä¸»é¢˜:** {topics_str}"
            # æ„å»ºè¯¦ç»†ä¿¡æ¯ï¼ˆMarkdownæ ¼å¼ï¼‰
            details_lines = [f"ğŸ” **æœç´¢ä¸»é¢˜:** {topics_str}"]

            if results:
                details_lines.append(
                    f"ğŸ¯ **ä½¿ç”¨ç­–ç•¥:** {results.get('search_strategy_used', 'æœªçŸ¥')}"
                )
                details_lines.append(
                    f"ğŸ“Š **æ€»å¯ç”¨è®ºæ–‡:** {results.get('total_results', paper_count)} ç¯‡"
                )

                # æ˜¾ç¤ºæ‰€æœ‰è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯ï¼ˆMarkdownæ ¼å¼ï¼‰
                if papers:
                    details_lines.append("\n## ğŸ“Š å‘ç°è®ºæ–‡")
                    for i, paper in enumerate(papers, 1):
                        title = paper.get("title", "æœªçŸ¥æ ‡é¢˜")
                        arxiv_id = paper.get("arxiv_id", "")
                        url = paper.get("url", "")

                        # ä½¿ç”¨Markdowné“¾æ¥æ ¼å¼
                        if url and arxiv_id:
                            details_lines.append(f"\n**{i}. [{title}]({url})**")
                            details_lines.append(f"ğŸ“„ **arXiv:** `{arxiv_id}`")
                        else:
                            details_lines.append(f"\n**{i}. {title}**")
                            if arxiv_id:
                                details_lines.append(f"ğŸ“„ **arXiv:** `{arxiv_id}`")

                        # ä½œè€…ä¿¡æ¯ï¼ˆæ˜¾ç¤ºæ‰€æœ‰ä½œè€…ï¼‰
                        if paper.get("authors"):
                            authors_str = ", ".join(paper["authors"])
                            details_lines.append(f"ğŸ‘¥ **ä½œè€…:** {authors_str}")

                        # æ‘˜è¦ä¿¡æ¯ï¼ˆå‰200å­—ç¬¦ï¼‰
                        if paper.get("abstract"):
                            abstract = paper["abstract"]

                            details_lines.append(f"ğŸ“ **æ‘˜è¦:** {abstract}")

                        # æäº¤æ—¥æœŸ
                        if paper.get("submitted_date"):
                            details_lines.append(
                                f"ğŸ“… **æäº¤æ—¥æœŸ:** {paper['submitted_date']}"
                            )

                        # å­¦ç§‘åˆ†ç±»ï¼ˆæ˜¾ç¤ºæ‰€æœ‰åˆ†ç±»ï¼‰
                        if paper.get("subjects"):
                            subjects_str = ", ".join(
                                [f"`{s}`" for s in paper["subjects"]]
                            )
                            details_lines.append(f"ğŸ·ï¸ **é¢†åŸŸ:** {subjects_str}")

                        # è¯„è®ºä¿¡æ¯
                        if paper.get("comments"):
                            comments = paper["comments"]

                            details_lines.append(f"ğŸ’¬ **è¯„è®º:** {comments}")

                        details_lines.append("---")  # åˆ†éš”çº¿

            details_lines.append(
                f"\nâ° **æ‰§è¡Œæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            )
            details = "\n".join(details_lines)

        # åˆ›å»ºä»»åŠ¡ï¼ˆæ”¯æŒåŒè¯­ç¿»è¯‘ï¼‰
        bilingual_enabled = DIDA_API_CONFIG.get("enable_bilingual", False)
        result = create_arxiv_task(
            report_type="topic",
            summary=summary,
            details=details,
            paper_count=paper_count,
            bilingual=bilingual_enabled,
        )

        if result.get("success"):
            print("âœ… æ»´ç­”æ¸…å•ä»»åŠ¡åˆ›å»ºæˆåŠŸ!")
            if result.get("task_id"):
                print(f"   ä»»åŠ¡ID: {result['task_id']}")
            if result.get("url"):
                print(f"   ä»»åŠ¡é“¾æ¥: {result['url']}")
        else:
            print(f"âŒ æ»´ç­”æ¸…å•ä»»åŠ¡åˆ›å»ºå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    except Exception as e:
        print(f"âŒ åˆ›å»ºæ»´ç­”æ¸…å•ä»»åŠ¡æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°"""
    # é»˜è®¤æœç´¢ AI + å®‰å…¨/å¯†ç å­¦ äº¤å‰é¢†åŸŸ
    topics = ["cs.AI", "cs.CR"]
    results = None

    try:
        print("ğŸ” åŸºäºä¸»é¢˜çš„è®ºæ–‡æœç´¢ç³»ç»Ÿ")
        print("=" * 50)

        # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–è€…ç›´æ¥ä¿®æ”¹æ¥è‡ªå®šä¹‰
        import sys

        if len(sys.argv) > 1:
            # æ”¯æŒå‘½ä»¤è¡Œè¾“å…¥ä¸»é¢˜
            topics = sys.argv[1].split(",")
            topics = [topic.strip() for topic in topics]  # æ¸…ç†ç©ºæ ¼

        print(f"ğŸ“š æœç´¢ä¸»é¢˜: {' AND '.join(topics)}")

        # æ£€æµ‹æ˜¯å¦åœ¨CIç¯å¢ƒä¸­è¿è¡Œ
        is_ci = os.getenv("GITHUB_ACTIONS") == "true"

        if is_ci:
            # CIç¯å¢ƒï¼šè¿è¡Œå•ä¸€çš„æ™ºèƒ½æœç´¢ï¼Œå‡å°‘è¾“å‡º
            print("\nğŸ” CIæ¨¡å¼: æ™ºèƒ½æœç´¢æœ€æ–°è®ºæ–‡")
            results = get_topic_papers_with_smart_dates(topics, days_back=3)
            display_search_results(results, limit=10)
        else:
            # æœ¬åœ°ç¯å¢ƒï¼šè¿è¡Œå®Œæ•´çš„æµ‹è¯•æ¨¡å¼
            print("\nğŸ” æµ‹è¯•1: æ™ºèƒ½æœç´¢æœ€è¿‘3å¤©çš„è®ºæ–‡")
            results1 = get_topic_papers_with_smart_dates(topics, days_back=3)
            display_search_results(results1, limit=5)

            print("\n\nğŸ” æµ‹è¯•2: ä¸é™æ—¥æœŸæœç´¢ï¼ˆè·å–æœ€æ–°50ç¯‡ï¼‰")
            results2 = fetch_papers_by_topic(topics, date_from=None, date_to=None)
            display_search_results(results2, limit=10)

            results = results2 if results2["papers"] else results1

        # åˆ›å»ºæ»´ç­”æ¸…å•ä»»åŠ¡
        create_topic_dida_task(topics, results)

        # ä¿å­˜æœ€æ–°çš„ç»“æœåˆ°æ–‡ä»¶
        output_file = (
            f"reports/topic_papers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        try:
            os.makedirs("reports", exist_ok=True)

            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            # CIç¯å¢ƒä¸­æ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
            if os.getenv("GITHUB_ACTIONS") == "true":
                papers_count = len(results.get("papers", []))
                strategy_used = results.get("search_strategy_used", "N/A")
                print("ğŸ“Š æœ¬æ¬¡æœç´¢æ€»ç»“:")
                print(f"   ğŸ¯ ç­–ç•¥: {strategy_used}")
                print(f"   ğŸ“„ è®ºæ–‡æ•°é‡: {papers_count}")
                print(f"   ğŸ·ï¸  ä¸»é¢˜: {' AND '.join(topics)}")

        except Exception as e:
            print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()
        # åˆ›å»ºé”™è¯¯è®°å½•ä»»åŠ¡
        create_topic_dida_task(topics, results, error=str(e))


if __name__ == "__main__":
    main()
