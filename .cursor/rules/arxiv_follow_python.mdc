---
description:
globs:
alwaysApply: false
---
---
description: ArXiv è®ºæ–‡ç›‘æ§ç³»ç»Ÿ Python ç¼–ç¨‹è§„èŒƒå’Œæœ€ä½³å®è·µ
alwaysApply: true
---

# ArXiv è®ºæ–‡ç›‘æ§ç³»ç»Ÿç¼–ç¨‹è§„èŒƒ

## ğŸ“ ä»£ç é£æ ¼å’Œæ³¨é‡Š

### ä¸­æ–‡æ³¨é‡Šè¦æ±‚
- **æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²**: ä½¿ç”¨ä¸­æ–‡ä¸‰å¼•å·æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œç®€æ´æè¿°æ¨¡å—åŠŸèƒ½
- **å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²**: ä½¿ç”¨ä¸­æ–‡ï¼ŒåŒ…å«Argsã€Returnsè¯´æ˜
- **è¡Œå†…æ³¨é‡Š**: é‡è¦é€»è¾‘ä½¿ç”¨ä¸­æ–‡æ³¨é‡Šè§£é‡Š
- **é…ç½®å’Œå¸¸é‡**: ä½¿ç”¨ä¸­æ–‡æ³¨é‡Šè¯´æ˜ç”¨é€”

```python
"""
æ¯æ—¥è®ºæ–‡ç›‘æ§è„šæœ¬ - æœç´¢ç ”ç©¶è€…å½“å¤©å‘å¸ƒçš„è®ºæ–‡
"""

def fetch_researchers_from_tsv(url: str) -> List[Dict[str, Any]]:
    """
    ä» TSV URL è·å–ç ”ç©¶è€…æ•°æ®
    
    Args:
        url: Google Sheets TSV å¯¼å‡ºé“¾æ¥
        
    Returns:
        ç ”ç©¶è€…æ•°æ®åˆ—è¡¨
    """
```

### ä»£ç ç»“æ„
- **å¯¼å…¥é¡ºåº**: æ ‡å‡†åº“ â†’ ç¬¬ä¸‰æ–¹åº“ â†’ æœ¬åœ°æ¨¡å—
- **ç±»å‹æ³¨è§£**: å¿…é¡»ä½¿ç”¨ç±»å‹æç¤ºï¼Œç‰¹åˆ«æ˜¯å‡½æ•°å‚æ•°å’Œè¿”å›å€¼
- **å¸¸é‡å‘½å**: ä½¿ç”¨å¤§å†™å­—æ¯å’Œä¸‹åˆ’çº¿
- **é…ç½®é›†ä¸­**: æ‰€æœ‰é…ç½®æ”¾åœ¨config.pyä¸­
- **TDDå¼€å‘**: å¿…é¡»å…ˆå†™æµ‹è¯•å†å†™å®ç°ä»£ç 
- **ä»£ç è´¨é‡**: æ‰€æœ‰ä»£ç å¿…é¡»é€šè¿‡ `ruff check` æ£€æŸ¥

## ğŸ”§ æŠ€æœ¯æ ˆè§„èŒƒ

### HTTP è¯·æ±‚ (httpx)
- ä½¿ç”¨httpxåº“è¿›è¡ŒHTTPè¯·æ±‚
- è®¾ç½®è¶…æ—¶æ—¶é—´å’Œé‡è¯•æœºåˆ¶
- ä½¿ç”¨context managerç®¡ç†è¿æ¥
- å¤„ç†é‡å®šå‘å’Œé”™è¯¯çŠ¶æ€

```python
with httpx.Client(follow_redirects=True, timeout=REQUEST_TIMEOUT) as client:
    response = client.get(url)
    response.raise_for_status()
```

### é”™è¯¯å¤„ç†
- ä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹
- æä¾›ä¸­æ–‡é”™è¯¯ä¿¡æ¯
- è®°å½•é”™è¯¯æ—¥å¿—ä¾¿äºè°ƒè¯•
- ä¼˜é›…é™çº§ï¼Œä¸è®©å•ä¸ªå¤±è´¥å½±å“æ•´ä½“æµç¨‹

```python
try:
    # æ ¸å¿ƒé€»è¾‘
    pass
except httpx.RequestError as e:
    print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
    return []
except Exception as e:
    print(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
    return []
```

### æ•°æ®å¤„ç†
- ä½¿ç”¨typingæ¨¡å—è¿›è¡Œç±»å‹å£°æ˜
- å­—å…¸å’Œåˆ—è¡¨ä½¿ç”¨ç±»å‹æ³¨è§£
- å¤„ç†ç©ºå€¼å’Œè¾¹ç•Œæƒ…å†µ
- æ•°æ®éªŒè¯å’Œæ¸…æ´—

```python
from typing import List, Dict, Any, Optional

def process_data(raw_data: List[str]) -> Dict[str, Any]:
    """å¤„ç†åŸå§‹æ•°æ®å¹¶è¿”å›ç»“æ„åŒ–ç»“æœ"""
```

## ğŸŒ Web çˆ¬è™«å’Œè§£æ

### HTML è§£æ
- ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æHTML (ç°æœ‰æ¨¡å¼)
- æä¾›å¤šç§è§£ææ¨¡å¼ä½œä¸ºfallback
- å¤„ç†HTMLç¼–ç å’Œç‰¹æ®Šå­—ç¬¦
- æå–å…³é”®ä¿¡æ¯ï¼šæ ‡é¢˜ã€ä½œè€…ã€é“¾æ¥ã€ID

```python
# å°è¯•å¤šç§è§£ææ¨¡å¼
title_patterns = [
    r'<p class="title is-5 mathjax">\s*<a[^>]*>(.*?)</a>',
    r'<span class="title"[^>]*>(.*?)</span>',
    # ... æ›´å¤šæ¨¡å¼
]

for pattern in title_patterns:
    match = re.search(pattern, html_content, re.DOTALL)
    if match:
        title = re.sub(r'<[^>]+>', '', match.group(1)).strip()
        break
```

### URL æ„å»º
- ä½¿ç”¨urllib.parse.urlencodeæ„å»ºæŸ¥è¯¢å‚æ•°
- å‚æ•°åŒ–é…ç½®æœç´¢æ¡ä»¶
- æ”¯æŒæ—¥æœŸèŒƒå›´å’Œå¤šå­—æ®µæœç´¢

## ğŸ”Œ API é›†æˆ

### å¤–éƒ¨ API è°ƒç”¨
- é›†æˆå¤±è´¥æ—¶æä¾›fallbackæœºåˆ¶
- ä½¿ç”¨å¯é€‰å¯¼å…¥å¤„ç†ä¾èµ–ç¼ºå¤±
- APIé…ç½®é›†ä¸­ç®¡ç†
- æä¾›è¯¦ç»†çš„çŠ¶æ€åé¦ˆ

```python
# ä¼˜é›…å¤„ç†å¯é€‰åŠŸèƒ½
try:
    from dida_integration import create_arxiv_task
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥æ»´ç­”æ¸…å•é›†æˆæ¨¡å—ï¼Œç›¸å…³åŠŸèƒ½å°†è¢«ç¦ç”¨")
    def create_arxiv_task(*args, **kwargs):
        return {"success": False, "error": "æ¨¡å—æœªå¯¼å…¥"}
```

### é…ç½®ç®¡ç†
- æ‰€æœ‰é…ç½®æ”¾åœ¨config.pyä¸­
- ä½¿ç”¨å­—å…¸ç»“æ„ç»„ç»‡ç›¸å…³é…ç½®
- æä¾›é»˜è®¤å€¼å’Œå¤‡é€‰æ–¹æ¡ˆ
- æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

## ğŸ“Š æ•°æ®è¾“å‡ºå’Œæ˜¾ç¤º

### æ§åˆ¶å°è¾“å‡º
- ä½¿ç”¨Emojiå¢å¼ºå¯è¯»æ€§
- ä¸­æ–‡æç¤ºä¿¡æ¯
- ç»“æ„åŒ–æ˜¾ç¤ºç»“æœ
- æä¾›ç»Ÿè®¡ä¿¡æ¯

```python
print("ğŸ” æ¯æ—¥è®ºæ–‡ç›‘æ§ - è·å–ç ”ç©¶è€…å½“å¤©å‘å¸ƒçš„è®ºæ–‡")
print(f"âœ… æ‰¾åˆ° {len(papers)} ç¯‡æ–°è®ºæ–‡!")
```

### æ—¥æœŸå¤„ç†
- ä½¿ç”¨datetimeæ¨¡å—
- æ”¯æŒæ—¥æœŸå›é€€ç­–ç•¥
- å¤„ç†æ—¶åŒºè½¬æ¢
- æ™ºèƒ½æ—¥æœŸèŒƒå›´è°ƒæ•´

## ğŸ§ª æµ‹è¯•é©±åŠ¨å¼€å‘å’Œè´¨é‡

### TDD å¼€å‘æµç¨‹ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰
- **çº¢-ç»¿-é‡æ„å¾ªç¯**: å…ˆå†™å¤±è´¥æµ‹è¯• â†’ æœ€å°ä»£ç å®ç° â†’ é‡æ„ä¼˜åŒ–
- **æµ‹è¯•å…ˆè¡Œ**: ä»»ä½•æ–°åŠŸèƒ½éƒ½å¿…é¡»å…ˆå†™æµ‹è¯•
- **å°æ­¥è¿­ä»£**: æ¯æ¬¡åªå®ç°ä¸€ä¸ªå°åŠŸèƒ½ç‚¹

```python
# ç¤ºä¾‹ï¼šæ·»åŠ æ—¥æœŸå›é€€åŠŸèƒ½çš„TDDæµç¨‹

# 1. å…ˆå†™æµ‹è¯•ï¼ˆçº¢è‰²é˜¶æ®µï¼‰
def test_search_with_date_fallback():
    """æµ‹è¯•æœç´¢å¤±è´¥æ—¶çš„æ—¥æœŸå›é€€ç­–ç•¥"""
    result = search_papers_with_fallback("cs.AI", "2025-01-01")
    assert result["success"] == True
    assert result["fallback_days"] > 0  # æœŸæœ›ä½¿ç”¨äº†å›é€€ç­–ç•¥

# 2. æœ€å°å®ç°ï¼ˆç»¿è‰²é˜¶æ®µï¼‰
def search_papers_with_fallback(topic, date):
    return {"success": True, "fallback_days": 7}

# 3. å®Œæ•´å®ç°ï¼ˆé‡æ„é˜¶æ®µï¼‰
def search_papers_with_fallback(topic, date):
    # å®ç°çœŸæ­£çš„æœç´¢å’Œå›é€€é€»è¾‘
    papers = search_papers(topic, date)
    if not papers:
        papers = search_papers(topic, get_fallback_date(date, 7))
        return {"success": True, "papers": papers, "fallback_days": 7}
    return {"success": True, "papers": papers, "fallback_days": 0}
```

### ä»£ç è´¨é‡è¦æ±‚
- **Ruffæ£€æŸ¥**: æ‰€æœ‰ä»£ç å¿…é¡»é€šè¿‡ `ruff check` æ— é”™è¯¯
- **æµ‹è¯•è¦†ç›–**: æ ¸å¿ƒåŠŸèƒ½å¿…é¡»æœ‰å¯¹åº”æµ‹è¯•
- **ç±»å‹æ£€æŸ¥**: å»ºè®®ä½¿ç”¨mypyè¿›è¡Œç±»å‹éªŒè¯

### ä»£ç ç»„ç»‡
- æ¯ä¸ªä¸»è¦åŠŸèƒ½ç‹¬ç«‹æ–‡ä»¶
- å…±äº«é…ç½®å’Œå·¥å…·å‡½æ•°å¤ç”¨
- æ¨¡å—èŒè´£å•ä¸€æ˜ç¡®
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°

## ğŸš€ éƒ¨ç½²å’Œè‡ªåŠ¨åŒ–

### GitHub Actions
- æ”¯æŒå®šæ—¶æ‰§è¡Œ
- ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿä¿¡æ¯
- é”™è¯¯å¤„ç†å’Œé€šçŸ¥
- æ”¯æŒæ‰‹åŠ¨è§¦å‘

### ä¾èµ–ç®¡ç†
- ä½¿ç”¨uvä½œä¸ºåŒ…ç®¡ç†å™¨
- pyproject.tomlé…ç½®é¡¹ç›®ä¿¡æ¯
- æœ€å°åŒ–ä¾èµ–ï¼ŒåªåŒ…å«å¿…éœ€åº“
- æŒ‡å®šPythonç‰ˆæœ¬è¦æ±‚

## ğŸ’¡ ç‰¹å®šä¸šåŠ¡é€»è¾‘

### ArXiv æœç´¢ä¼˜åŒ–
- æ™ºèƒ½æ—¥æœŸå›é€€ç­–ç•¥ï¼ˆç²¾ç¡®æ—¥æœŸ â†’ 7å¤© â†’ 30å¤© â†’ ä¸é™æ—¥æœŸï¼‰
- å¤šä¸»é¢˜ç»„åˆæœç´¢æ”¯æŒ
- è®ºæ–‡å»é‡å’Œæ’åº
- æœç´¢ç»“æœç»Ÿè®¡

### æ•°æ®æºé›†æˆ
- Google Sheets TSVè§£æ
- çµæ´»çš„æ•°æ®æ ¼å¼å¤„ç†
- è‡ªåŠ¨æ£€æµ‹è¡¨å¤´å’Œæ•°æ®è¡Œ
- æ”¯æŒå¤šåˆ—å’Œå•åˆ—æ ¼å¼

å½“ç¼–å†™è¿™ä¸ªé¡¹ç›®çš„ä»£ç æ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªä»¥ä¸Šè§„èŒƒï¼Œç¡®ä¿ä»£ç çš„ä¸€è‡´æ€§ã€å¯ç»´æŠ¤æ€§å’Œä¸­æ–‡å‹å¥½æ€§ã€‚
