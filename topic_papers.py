#!/usr/bin/env python3
"""
åŸºäºä¸»é¢˜çš„è®ºæ–‡ç›‘æ§è„šæœ¬ - æœç´¢ç‰¹å®šä¸»é¢˜é¢†åŸŸçš„æœ€æ–°è®ºæ–‡
æ”¯æŒæ™ºèƒ½æ—¥æœŸå›é€€å’Œå¤šç§æœç´¢æ¨¡å¼
"""

import httpx
import os
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import re
from urllib.parse import urlencode
import json

# å¯¼å…¥æ»´ç­”æ¸…å•é›†æˆ
try:
    from dida_integration import create_arxiv_task
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥æ»´ç­”æ¸…å•é›†æˆæ¨¡å—ï¼Œç›¸å…³åŠŸèƒ½å°†è¢«ç¦ç”¨")
    def create_arxiv_task(*args, **kwargs):
        return {"success": False, "error": "æ¨¡å—æœªå¯¼å…¥"}


def build_topic_search_url(
    topics: List[str], 
    date_from: Optional[str] = None, 
    date_to: Optional[str] = None,
    classification: str = "computer_science",
    field: str = "all",
    size: int = 50
) -> str:
    """
    æ„å»ºåŸºäºä¸»é¢˜çš„ arXiv é«˜çº§æœç´¢ URL
    
    Args:
        topics: ä¸»é¢˜åˆ—è¡¨ (å¦‚ ["cs.AI", "cs.CR"])
        date_from: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD), None è¡¨ç¤ºä¸é™åˆ¶
        date_to: ç»“æŸæ—¥æœŸ (YYYY-MM-DD), None è¡¨ç¤ºä¸é™åˆ¶
        classification: åˆ†ç±»é¢†åŸŸ
        field: æœç´¢å­—æ®µ
        size: ç»“æœæ•°é‡
        
    Returns:
        arXiv æœç´¢ URL
    """
    base_url = "https://arxiv.org/search/advanced"
    
    params = {
        'advanced': '',
        'abstracts': 'show',
        'size': str(size),
        'order': '-announced_date_first'
    }
    
    # æ·»åŠ ä¸»é¢˜æœç´¢æ¡ä»¶
    for i, topic in enumerate(topics):
        params[f'terms-{i}-operator'] = 'AND'
        params[f'terms-{i}-term'] = topic
        params[f'terms-{i}-field'] = field
    
    # æ·»åŠ åˆ†ç±»
    if classification == "computer_science":
        params['classification-computer_science'] = 'y'
    elif classification == "physics":
        params['classification-physics_archives'] = 'all'
    
    params['classification-include_cross_list'] = 'include'
    
    # æ·»åŠ æ—¥æœŸæ¡ä»¶
    if date_from and date_to:
        params['date-filter_by'] = 'date_range'
        params['date-from_date'] = date_from
        params['date-to_date'] = date_to
        params['date-date_type'] = 'submitted_date'
    else:
        params['date-filter_by'] = 'all_dates'
    
    params['date-year'] = ''
    
    return f"{base_url}?{urlencode(params)}"


def parse_arxiv_search_results(html_content: str) -> List[Dict[str, Any]]:
    """
    è§£æ arXiv æœç´¢ç»“æœé¡µé¢
    
    Args:
        html_content: HTML å†…å®¹
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
    """
    papers = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
    if "Sorry, your query returned no results" in html_content:
        return papers
    
    # æå–ç»“æœæ€»æ•°
    total_pattern = r'Showing 1â€“\d+ of ([\d,]+) results'
    total_match = re.search(total_pattern, html_content)
    total_count = 0
    if total_match:
        total_count = int(total_match.group(1).replace(',', ''))
    
    # æŸ¥æ‰¾è®ºæ–‡æ¡ç›® - ä½¿ç”¨å®é™…çš„HTMLç»“æ„
    paper_pattern = r'<li class="arxiv-result">(.*?)</li>'
    paper_matches = re.findall(paper_pattern, html_content, re.DOTALL)
    
    for match in paper_matches:
        paper = {
            'total_results': total_count
        }
        
        # æå–arXiv IDå’ŒURL
        id_pattern = r'<a href="https://arxiv\.org/abs/(\d{4}\.\d{4,5})">arXiv:(\d{4}\.\d{4,5})</a>'
        id_match = re.search(id_pattern, match)
        if id_match:
            paper['arxiv_id'] = id_match.group(1)
            paper['url'] = f"https://arxiv.org/abs/{paper['arxiv_id']}"
        
        # æå–æ ‡é¢˜
        title_pattern = r'<p class="title is-5 mathjax"[^>]*>\s*(.*?)\s*</p>'
        title_match = re.search(title_pattern, match, re.DOTALL)
        if title_match:
            title = title_match.group(1).strip()
            # æ¸…ç†HTMLæ ‡ç­¾
            title = re.sub(r'<[^>]+>', '', title)
            title = re.sub(r'\s+', ' ', title).strip()
            if title:
                paper['title'] = title
        
        # æå–ä½œè€…
        authors_pattern = r'<p class="authors"[^>]*>.*?<span[^>]+>Authors:</span>(.*?)</p>'
        authors_match = re.search(authors_pattern, match, re.DOTALL)
        if authors_match:
            authors_html = authors_match.group(1)
            # æå–æ‰€æœ‰ä½œè€…é“¾æ¥
            author_links = re.findall(r'<a[^>]+>(.*?)</a>', authors_html)
            if author_links:
                authors = [re.sub(r'<[^>]+>', '', author).strip() for author in author_links]
                authors = [author for author in authors if author]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
                if authors:
                    paper['authors'] = authors
        
        # æå–å­¦ç§‘åˆ†ç±»
        subjects = []
        subject_pattern = r'<span class="tag[^"]*"[^>]*data-tooltip="([^"]+)"[^>]*>([^<]+)</span>'
        subject_matches = re.findall(subject_pattern, match)
        for tooltip, subject_code in subject_matches:
            subjects.append(subject_code.strip())
        if subjects:
            paper['subjects'] = subjects
        
        # æå–æ‘˜è¦ - ä¼˜å…ˆè·å–å®Œæ•´æ‘˜è¦
        abstract_patterns = [
            # ä¼˜å…ˆæå–å®Œæ•´æ‘˜è¦
            r'<span[^>]*class="[^"]*abstract-full[^"]*"[^>]*[^>]*>(.*?)</span>',
            # å¤‡é€‰ï¼šæ™®é€šæ‘˜è¦æ®µè½
            r'<p[^>]*class="[^"]*abstract[^"]*"[^>]*>.*?<span[^>]+>Abstract[^<]*</span>:\s*(.*?)</p>',
            # å¤‡é€‰ï¼šabstract-shortï¼ˆå¦‚æœæ²¡æœ‰fullç‰ˆæœ¬ï¼‰
            r'<span[^>]*class="[^"]*abstract-short[^"]*"[^>]*[^>]*>(.*?)</span>',
        ]
        
        for pattern in abstract_patterns:
            abstract_match = re.search(pattern, match, re.DOTALL | re.IGNORECASE)
            if abstract_match:
                abstract = abstract_match.group(1).strip()
                # æ¸…ç†HTMLæ ‡ç­¾ã€é“¾æ¥å’Œå¤šä½™ç©ºç™½
                abstract = re.sub(r'<a[^>]*>.*?</a>', '', abstract)  # ç§»é™¤More/Lessé“¾æ¥
                abstract = re.sub(r'<[^>]+>', '', abstract)
                abstract = re.sub(r'&hellip;.*', '', abstract)  # ç§»é™¤çœç•¥å·åŠåç»­å†…å®¹
                abstract = re.sub(r'\s+', ' ', abstract).strip()
                if len(abstract) > 20:  # ç¡®ä¿ä¸æ˜¯ç©ºçš„æˆ–å¤ªçŸ­çš„å†…å®¹
                    paper['abstract'] = abstract
                    break
        
        # æå–æäº¤æ—¥æœŸ
        submitted_pattern = r'<span[^>]+>Submitted</span>\s+([^;]+);'
        submitted_match = re.search(submitted_pattern, match)
        if submitted_match:
            paper['submitted_date'] = submitted_match.group(1).strip()
        
        # æå–è¯„è®ºä¿¡æ¯
        comments_pattern = r'<p class="comments[^"]*"[^>]*>.*?<span[^>]+>Comments:</span>\s*<span[^>]*>(.*?)</span>'
        comments_match = re.search(comments_pattern, match, re.DOTALL)
        if comments_match:
            comments = re.sub(r'<[^>]+>', '', comments_match.group(1)).strip()
            if comments:
                paper['comments'] = comments
        
        # åªæ·»åŠ è‡³å°‘æœ‰æ ‡é¢˜æˆ–arXiv IDçš„è®ºæ–‡
        if paper.get('title') or paper.get('arxiv_id'):
            papers.append(paper)
    
    return papers


def fetch_papers_by_topic(
    topics: List[str], 
    date_from: Optional[str] = None, 
    date_to: Optional[str] = None,
    max_retries: int = 3
) -> Dict[str, Any]:
    """
    æ ¹æ®ä¸»é¢˜æœç´¢è®ºæ–‡ï¼Œæ”¯æŒæ™ºèƒ½æ—¥æœŸå›é€€
    
    Args:
        topics: ä¸»é¢˜åˆ—è¡¨
        date_from: å¼€å§‹æ—¥æœŸ
        date_to: ç»“æŸæ—¥æœŸ  
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        åŒ…å«è®ºæ–‡åˆ—è¡¨å’Œæœç´¢ä¿¡æ¯çš„å­—å…¸
    """
    
    # å®šä¹‰æœç´¢ç­–ç•¥
    search_strategies = []
    
    if date_from and date_to:
        # ç­–ç•¥1: ç²¾ç¡®æ—¥æœŸèŒƒå›´
        search_strategies.append({
            'name': f'ç²¾ç¡®æ—¥æœŸèŒƒå›´ ({date_from} åˆ° {date_to})',
            'date_from': date_from,
            'date_to': date_to
        })
        
        # ç­–ç•¥2: æ‰©å±•åˆ°æœ€è¿‘7å¤©
        try:
            end_date = datetime.strptime(date_to, '%Y-%m-%d')
            start_date = end_date - timedelta(days=7)
            search_strategies.append({
                'name': f'æœ€è¿‘7å¤© ({start_date.strftime("%Y-%m-%d")} åˆ° {date_to})',
                'date_from': start_date.strftime('%Y-%m-%d'),
                'date_to': date_to
            })
        except:
            pass
        
        # ç­–ç•¥3: æ‰©å±•åˆ°æœ€è¿‘30å¤©
        try:
            end_date = datetime.strptime(date_to, '%Y-%m-%d')
            start_date = end_date - timedelta(days=30)
            search_strategies.append({
                'name': f'æœ€è¿‘30å¤© ({start_date.strftime("%Y-%m-%d")} åˆ° {date_to})',
                'date_from': start_date.strftime('%Y-%m-%d'),
                'date_to': date_to
            })
        except:
            pass
    
    # ç­–ç•¥4: ä¸é™æ—¥æœŸ
    search_strategies.append({
        'name': 'ä¸é™æ—¥æœŸ',
        'date_from': None,
        'date_to': None
    })
    
    results = {
        'topics': topics,
        'papers': [],
        'search_strategy_used': None,
        'total_results': 0,
        'search_url': None,
        'attempted_strategies': []
    }
    
    # å°è¯•å„ç§æœç´¢ç­–ç•¥
    for strategy in search_strategies:
        try:
            print(f"ğŸ” å°è¯•æœç´¢ç­–ç•¥: {strategy['name']}")
            
            url = build_topic_search_url(
                topics=topics,
                date_from=strategy['date_from'],
                date_to=strategy['date_to']
            )
            
            print(f"ğŸŒ æœç´¢URL: {url}")
            
            with httpx.Client(follow_redirects=True, timeout=30.0) as client:
                response = client.get(url)
                response.raise_for_status()
                
            papers = parse_arxiv_search_results(response.text)
            
            strategy_result = {
                'name': strategy['name'],
                'papers_found': len(papers),
                'total_available': papers[0].get('total_results', 0) if papers else 0,
                'url': url
            }
            results['attempted_strategies'].append(strategy_result)
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            
            if papers:
                results['papers'] = papers
                results['search_strategy_used'] = strategy['name']
                results['total_results'] = papers[0].get('total_results', len(papers))
                results['search_url'] = url
                break
            else:
                print(f"âŒ è¯¥ç­–ç•¥æœªæ‰¾åˆ°ç»“æœï¼Œå°è¯•ä¸‹ä¸€ä¸ªç­–ç•¥...")
                
        except Exception as e:
            print(f"âŒ æœç´¢ç­–ç•¥ '{strategy['name']}' å¤±è´¥: {e}")
            results['attempted_strategies'].append({
                'name': strategy['name'],
                'error': str(e),
                'url': url if 'url' in locals() else None
            })
            continue
    
    return results


def display_search_results(results: Dict[str, Any], limit: int = 10) -> None:
    """
    æ˜¾ç¤ºæœç´¢ç»“æœ
    
    Args:
        results: æœç´¢ç»“æœå­—å…¸
        limit: æ˜¾ç¤ºè®ºæ–‡æ•°é‡é™åˆ¶
    """
    print("\n" + "="*80)
    print(f"ğŸ” ä¸»é¢˜æœç´¢ç»“æœ")
    print(f"ğŸ·ï¸  æœç´¢ä¸»é¢˜: {' AND '.join(results['topics'])}")
    print(f"â° æœç´¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # æ˜¾ç¤ºå°è¯•çš„æœç´¢ç­–ç•¥
    print(f"\nğŸ“‹ æœç´¢ç­–ç•¥å°è¯•è®°å½•:")
    for i, strategy in enumerate(results['attempted_strategies'], 1):
        if 'error' in strategy:
            print(f"  {i}. âŒ {strategy['name']}: {strategy['error']}")
        else:
            print(f"  {i}. {'âœ…' if strategy['papers_found'] > 0 else 'âŒ'} {strategy['name']}: {strategy['papers_found']} ç¯‡è®ºæ–‡ (æ€»è®¡ {strategy['total_available']} ç¯‡)")
    
    if not results['papers']:
        print(f"\nâŒ æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½æœªæ‰¾åˆ°ç»“æœ")
        return
    
    print(f"\nğŸ¯ ä½¿ç”¨ç­–ç•¥: {results['search_strategy_used']}")
    print(f"ğŸ“Š æ˜¾ç¤ºå‰ {min(limit, len(results['papers']))} ç¯‡è®ºæ–‡ (æ€»è®¡ {results['total_results']} ç¯‡)")
    print(f"ğŸ”— æœç´¢é“¾æ¥: {results['search_url']}")
    
    # æ˜¾ç¤ºè®ºæ–‡åˆ—è¡¨
    for i, paper in enumerate(results['papers'][:limit], 1):
        print(f"\n{'-'*60}")
        print(f"ğŸ“„ {i}. {paper.get('title', 'æ— æ ‡é¢˜')}")
        print(f"ğŸ†” arXiv ID: {paper['arxiv_id']}")
        print(f"ğŸ·ï¸  å­¦ç§‘åˆ†ç±»: {', '.join(paper.get('subjects', []))}")
        
        if paper.get('authors'):
            authors_display = ', '.join(paper['authors'][:3])
            if len(paper['authors']) > 3:
                authors_display += f" ç­‰ {len(paper['authors'])} ä½ä½œè€…"
            print(f"ğŸ‘¥ ä½œè€…: {authors_display}")
        
        if paper.get('submitted_date'):
            print(f"ğŸ“… æäº¤æ—¥æœŸ: {paper['submitted_date']}")
            
        print(f"ğŸŒ é“¾æ¥: {paper['url']}")
        
        if paper.get('abstract'):
            abstract = paper['abstract']
            print(f"ğŸ“ æ‘˜è¦: {abstract}")
    
    if len(results['papers']) > limit:
        print(f"\nğŸ’¡ è¿˜æœ‰ {len(results['papers']) - limit} ç¯‡è®ºæ–‡æœªæ˜¾ç¤ºï¼Œå¯è°ƒæ•´ limit å‚æ•°æŸ¥çœ‹æ›´å¤š")


def get_topic_papers_with_smart_dates(
    topics: List[str], 
    target_date: Optional[str] = None,
    days_back: int = 1
) -> Dict[str, Any]:
    """
    æ™ºèƒ½è·å–ä¸»é¢˜è®ºæ–‡ï¼Œæ”¯æŒæ—¥æœŸå›é€€
    
    Args:
        topics: ä¸»é¢˜åˆ—è¡¨
        target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼ŒNone è¡¨ç¤ºä»Šå¤©
        days_back: å›é€€å¤©æ•°
        
    Returns:
        æœç´¢ç»“æœå­—å…¸
    """
    if target_date is None:
        target_date = datetime.now().strftime('%Y-%m-%d')
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    try:
        end_date = datetime.strptime(target_date, '%Y-%m-%d')
        start_date = end_date - timedelta(days=days_back-1)
        date_from = start_date.strftime('%Y-%m-%d')
        date_to = end_date.strftime('%Y-%m-%d')
    except:
        date_from = None
        date_to = None
    
    return fetch_papers_by_topic(topics, date_from, date_to)


def create_topic_dida_task(topics: List[str], 
                          results: Dict[str, Any],
                          error: str = None) -> None:
    """
    åˆ›å»ºä¸»é¢˜è®ºæ–‡æœç´¢çš„æ»´ç­”æ¸…å•ä»»åŠ¡
    
    Args:
        topics: æœç´¢ä¸»é¢˜åˆ—è¡¨
        results: æœç´¢ç»“æœå­—å…¸
        error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    """
    print("\nğŸ“ åˆ›å»ºæ»´ç­”æ¸…å•ä»»åŠ¡...")
    
    try:
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        papers = results.get('papers', []) if results else []
        paper_count = len(papers)
        
        # æ„å»ºä»»åŠ¡æ‘˜è¦
        topics_str = ' AND '.join(topics)
        if error:
            summary = f"âŒ ä¸»é¢˜è®ºæ–‡æœç´¢æ‰§è¡Œå¤±è´¥\nä¸»é¢˜: {topics_str}\né”™è¯¯ä¿¡æ¯: {error}"
            details = f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        elif paper_count == 0:
            summary = f"ğŸ¯ ä¸»é¢˜è®ºæ–‡æœç´¢æœªå‘ç°æ–°è®ºæ–‡\nä¸»é¢˜: {topics_str}"
            details = f"æœç´¢ä¸»é¢˜: {topics_str}\n"
            if results:
                strategy_info = []
                for strategy in results.get('attempted_strategies', []):
                    if 'error' in strategy:
                        strategy_info.append(f"âŒ {strategy['name']}: {strategy['error']}")
                    else:
                        strategy_info.append(f"â€¢ {strategy['name']}: {strategy['papers_found']} ç¯‡")
                details += f"å°è¯•ç­–ç•¥:\n" + "\n".join(strategy_info)
            details += f"\n\nâ° æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        else:
            summary = f"ğŸ‰ ä¸»é¢˜è®ºæ–‡æœç´¢å‘ç° {paper_count} ç¯‡è®ºæ–‡ï¼\nä¸»é¢˜: {topics_str}"
            # æ„å»ºè¯¦ç»†ä¿¡æ¯
            details_lines = [f"æœç´¢ä¸»é¢˜: {topics_str}"]
            
            if results:
                details_lines.append(f"ä½¿ç”¨ç­–ç•¥: {results.get('search_strategy_used', 'æœªçŸ¥')}")
                details_lines.append(f"æ€»å¯ç”¨è®ºæ–‡: {results.get('total_results', paper_count)} ç¯‡")
                
                # æ·»åŠ å‰3ç¯‡è®ºæ–‡æ ‡é¢˜
                if papers:
                    details_lines.append("\nğŸ“Š å‘ç°è®ºæ–‡:")
                    for i, paper in enumerate(papers[:3], 1):
                        title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
                        if len(title) > 60:
                            title = title[:60] + "..."
                        arxiv_id = paper.get('arxiv_id', '')
                        details_lines.append(f"{i}. {title} (arXiv:{arxiv_id})")
                    
                    if len(papers) > 3:
                        details_lines.append(f"... è¿˜æœ‰ {len(papers)-3} ç¯‡")
            
            details_lines.append(f"\nâ° æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            details = "\n".join(details_lines)
        
        # åˆ›å»ºä»»åŠ¡
        result = create_arxiv_task(
            report_type="topic",
            summary=summary,
            details=details,
            paper_count=paper_count
        )
        
        if result.get("success"):
            print(f"âœ… æ»´ç­”æ¸…å•ä»»åŠ¡åˆ›å»ºæˆåŠŸ!")
            if result.get("task_id"):
                print(f"   ä»»åŠ¡ID: {result['task_id']}")
            if result.get("url"):
                print(f"   ä»»åŠ¡é“¾æ¥: {result['url']}")
        else:
            print(f"âŒ æ»´ç­”æ¸…å•ä»»åŠ¡åˆ›å»ºå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ»´ç­”æ¸…å•ä»»åŠ¡æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°"""
    # é»˜è®¤æœç´¢ AI + å®‰å…¨/å¯†ç å­¦ äº¤å‰é¢†åŸŸ
    topics = ["cs.AI", "cs.CR"]
    results = None
    
    try:
        print("ğŸ” åŸºäºä¸»é¢˜çš„è®ºæ–‡æœç´¢ç³»ç»Ÿ")
        print("="*50)
        
        # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–è€…ç›´æ¥ä¿®æ”¹æ¥è‡ªå®šä¹‰
        import sys
        if len(sys.argv) > 1:
            # æ”¯æŒå‘½ä»¤è¡Œè¾“å…¥ä¸»é¢˜
            topics = sys.argv[1].split(',')
            topics = [topic.strip() for topic in topics]  # æ¸…ç†ç©ºæ ¼
        
        print(f"ğŸ“š æœç´¢ä¸»é¢˜: {' AND '.join(topics)}")
        
        # æ£€æµ‹æ˜¯å¦åœ¨CIç¯å¢ƒä¸­è¿è¡Œ
        is_ci = os.getenv('GITHUB_ACTIONS') == 'true'
        
        if is_ci:
            # CIç¯å¢ƒï¼šè¿è¡Œå•ä¸€çš„æ™ºèƒ½æœç´¢ï¼Œå‡å°‘è¾“å‡º
            print("\nğŸ” CIæ¨¡å¼: æ™ºèƒ½æœç´¢æœ€æ–°è®ºæ–‡")
            results = get_topic_papers_with_smart_dates(topics, days_back=3)
            display_search_results(results, limit=10)
        else:
            # æœ¬åœ°ç¯å¢ƒï¼šè¿è¡Œå®Œæ•´çš„æµ‹è¯•æ¨¡å¼
            print("\nğŸ” æµ‹è¯•1: æ™ºèƒ½æœç´¢æœ€è¿‘3å¤©çš„è®ºæ–‡")
            results1 = get_topic_papers_with_smart_dates(topics, days_back=3)
            display_search_results(results1, limit=5)
            
            print("\n\nğŸ” æµ‹è¯•2: ä¸é™æ—¥æœŸæœç´¢ï¼ˆè·å–æœ€æ–°50ç¯‡ï¼‰")
            results2 = fetch_papers_by_topic(topics, date_from=None, date_to=None)
            display_search_results(results2, limit=10)
            
            results = results2 if results2['papers'] else results1
        
        # åˆ›å»ºæ»´ç­”æ¸…å•ä»»åŠ¡
        create_topic_dida_task(topics, results)
        
        # ä¿å­˜æœ€æ–°çš„ç»“æœåˆ°æ–‡ä»¶
        output_file = f"reports/topic_papers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            os.makedirs("reports", exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # CIç¯å¢ƒä¸­æ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
            if os.getenv('GITHUB_ACTIONS') == 'true':
                papers_count = len(results.get('papers', []))
                strategy_used = results.get('search_strategy_used', 'N/A')
                print(f"ğŸ“Š æœ¬æ¬¡æœç´¢æ€»ç»“:")
                print(f"   ğŸ¯ ç­–ç•¥: {strategy_used}")
                print(f"   ğŸ“„ è®ºæ–‡æ•°é‡: {papers_count}")
                print(f"   ğŸ·ï¸  ä¸»é¢˜: {' AND '.join(topics)}")
                
        except Exception as e:
            print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        # åˆ›å»ºé”™è¯¯è®°å½•ä»»åŠ¡
        create_topic_dida_task(topics, results, error=str(e))


if __name__ == "__main__":
    main() 