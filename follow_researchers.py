#!/usr/bin/env python3
"""
ä» Google Sheets TSV é“¾æ¥è·å–ç ”ç©¶è€…åˆ—è¡¨å¹¶æ£€ç´¢ä»–ä»¬å½“å¤©å‘å¸ƒçš„è®ºæ–‡
"""

import httpx
import csv
import io
from typing import List, Dict, Any
from datetime import datetime, timedelta
import re
from urllib.parse import urlencode, quote


def fetch_researchers_from_tsv(url: str) -> List[Dict[str, Any]]:
    """
    ä» TSV URL è·å–ç ”ç©¶è€…æ•°æ®
    
    Args:
        url: Google Sheets TSV å¯¼å‡ºé“¾æ¥
        
    Returns:
        ç ”ç©¶è€…æ•°æ®åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨ httpx è·å– TSV æ•°æ®ï¼Œå…è®¸é‡å®šå‘
        with httpx.Client(follow_redirects=True) as client:
            response = client.get(url)
            response.raise_for_status()
            
        # è§£æ TSV æ•°æ®
        tsv_content = response.text
        print(f"è·å–åˆ°çš„åŸå§‹æ•°æ®:\n{tsv_content}\n")
        
        # ä½¿ç”¨ csv æ¨¡å—è§£æ TSV
        csv_reader = csv.reader(io.StringIO(tsv_content), delimiter='\t')
        
        # è¯»å–æ‰€æœ‰è¡Œ
        rows = list(csv_reader)
        
        if not rows:
            print("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®")
            return []
            
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜è¡Œï¼ˆé€šè¿‡æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦åŒ…å«æ˜æ˜¾çš„æ ‡é¢˜è¯æ±‡ï¼‰
        has_header = False
        if rows and len(rows) > 1:
            first_row = [cell.strip().lower() for cell in rows[0]]
            header_indicators = ['name', 'author', 'researcher', 'å§“å', 'ä½œè€…', 'ç ”ç©¶è€…']
            has_header = any(indicator in cell for cell in first_row for indicator in header_indicators)
        
        if has_header:
            headers = rows[0]
            data_rows = rows[1:]
            print(f"æ£€æµ‹åˆ°æ ‡é¢˜è¡Œ: {headers}")
        else:
            headers = []
            data_rows = rows
            print(f"æœªæ£€æµ‹åˆ°æ ‡é¢˜è¡Œï¼Œæ‰€æœ‰è¡Œéƒ½è§†ä¸ºæ•°æ®")
        
        print(f"æ•°æ®è¡Œæ•°: {len(data_rows)}")
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        researchers = []
        for i, row in enumerate(data_rows):
            if any(cell.strip() for cell in row):  # è·³è¿‡ç©ºè¡Œ
                if headers and len(headers) > 1:
                    # å¦‚æœæœ‰å¤šä¸ªåˆ—ï¼Œåˆ›å»ºå­—å…¸
                    researcher = {}
                    for j, header in enumerate(headers):
                        value = row[j] if j < len(row) else ""
                        researcher[header] = value.strip()
                    researchers.append(researcher)
                else:
                    # å¦‚æœåªæœ‰ä¸€åˆ—æˆ–æ²¡æœ‰æ ‡é¢˜ï¼Œå°†æ¯è¡Œä½œä¸ºç ”ç©¶è€…å§“å
                    name = " ".join(cell.strip() for cell in row if cell.strip())
                    if name:
                        researchers.append({"name": name, "row_index": i})
                        
        return researchers
        
    except httpx.RequestError as e:
        print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return []
    except Exception as e:
        print(f"è§£ææ•°æ®æ—¶å‡ºé”™: {e}")
        return []


def build_arxiv_search_url(author_name: str, date_from: str, date_to: str) -> str:
    """
    æ„å»º arXiv é«˜çº§æœç´¢ URL
    
    Args:
        author_name: ä½œè€…å§“å
        date_from: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        date_to: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        
    Returns:
        arXiv æœç´¢ URL
    """
    # ä¿®æ­£arXivæ—¥æœŸèŒƒå›´é—®é¢˜ï¼šå¦‚æœå¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸç›¸åŒï¼Œå°†ç»“æŸæ—¥æœŸè®¾ç½®ä¸ºç¬¬äºŒå¤©
    # å› ä¸ºarXivè¦æ±‚ "End date must be later than start date"
    if date_from == date_to:
        from datetime import datetime, timedelta
        try:
            start_date = datetime.strptime(date_from, '%Y-%m-%d')
            end_date = start_date + timedelta(days=1)
            date_to = end_date.strftime('%Y-%m-%d')
        except ValueError:
            # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œä¿æŒåŸæ ·
            pass
    
    base_url = "https://arxiv.org/search/advanced"
    
    params = {
        'advanced': '',
        'terms-0-operator': 'AND',
        'terms-0-term': f'"{author_name}"',
        'terms-0-field': 'author',
        'classification-computer_science': 'y',
        'classification-physics_archives': 'all',
        'classification-include_cross_list': 'include',
        'date-year': '',
        'date-filter_by': 'date_range',
        'date-from_date': date_from,
        'date-to_date': date_to,
        'date-date_type': 'submitted_date',
        'abstracts': 'show',
        'size': '50',
        'order': '-announced_date_first'
    }
    
    return f"{base_url}?{urlencode(params)}"


def parse_arxiv_search_results(html_content: str) -> List[Dict[str, Any]]:
    """
    è§£æ arXiv æœç´¢ç»“æœé¡µé¢
    
    Args:
        html_content: HTML å†…å®¹
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
    """
    papers = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
    if "Sorry, your query returned no results" in html_content:
        return papers
    
    # æå–ç»“æœæ€»æ•°
    total_pattern = r'Showing 1â€“\d+ of ([\d,]+) results'
    total_match = re.search(total_pattern, html_content)
    total_count = 0
    if total_match:
        total_count = int(total_match.group(1).replace(',', ''))
    
    # æŸ¥æ‰¾è®ºæ–‡æ¡ç›® - ä½¿ç”¨å®é™…çš„HTMLç»“æ„
    paper_pattern = r'<li class="arxiv-result">(.*?)</li>'
    paper_matches = re.findall(paper_pattern, html_content, re.DOTALL)
    
    for match in paper_matches:
        paper = {
            'total_results': total_count
        }
        
        # æå–arXiv IDå’ŒURL
        id_pattern = r'<a href="https://arxiv\.org/abs/(\d{4}\.\d{4,5})">arXiv:(\d{4}\.\d{4,5})</a>'
        id_match = re.search(id_pattern, match)
        if id_match:
            paper['arxiv_id'] = id_match.group(1)
            paper['url'] = f"https://arxiv.org/abs/{paper['arxiv_id']}"
        
        # æå–æ ‡é¢˜
        title_pattern = r'<p class="title is-5 mathjax"[^>]*>\s*(.*?)\s*</p>'
        title_match = re.search(title_pattern, match, re.DOTALL)
        if title_match:
            title = title_match.group(1).strip()
            # æ¸…ç†HTMLæ ‡ç­¾
            title = re.sub(r'<[^>]+>', '', title)
            title = re.sub(r'\s+', ' ', title).strip()
            if title:
                paper['title'] = title
        
        # æå–ä½œè€…
        authors_pattern = r'<p class="authors"[^>]*>.*?<span[^>]+>Authors:</span>(.*?)</p>'
        authors_match = re.search(authors_pattern, match, re.DOTALL)
        if authors_match:
            authors_html = authors_match.group(1)
            # æå–æ‰€æœ‰ä½œè€…é“¾æ¥
            author_links = re.findall(r'<a[^>]+>(.*?)</a>', authors_html)
            if author_links:
                authors = [re.sub(r'<[^>]+>', '', author).strip() for author in author_links]
                authors = [author for author in authors if author]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
                if authors:
                    paper['authors'] = authors
        
        # æå–å­¦ç§‘åˆ†ç±»
        subjects = []
        subject_pattern = r'<span class="tag[^"]*"[^>]*data-tooltip="([^"]+)"[^>]*>([^<]+)</span>'
        subject_matches = re.findall(subject_pattern, match)
        for tooltip, subject_code in subject_matches:
            subjects.append(subject_code.strip())
        if subjects:
            paper['subjects'] = subjects
        
        # æå–æ‘˜è¦ - ä¼˜å…ˆè·å–å®Œæ•´æ‘˜è¦
        abstract_patterns = [
            # ä¼˜å…ˆæå–å®Œæ•´æ‘˜è¦
            r'<span[^>]*class="[^"]*abstract-full[^"]*"[^>]*[^>]*>(.*?)</span>',
            # å¤‡é€‰ï¼šæ™®é€šæ‘˜è¦æ®µè½
            r'<p[^>]*class="[^"]*abstract[^"]*"[^>]*>.*?<span[^>]+>Abstract[^<]*</span>:\s*(.*?)</p>',
            # å¤‡é€‰ï¼šabstract-shortï¼ˆå¦‚æœæ²¡æœ‰fullç‰ˆæœ¬ï¼‰
            r'<span[^>]*class="[^"]*abstract-short[^"]*"[^>]*[^>]*>(.*?)</span>',
        ]
        
        for pattern in abstract_patterns:
            abstract_match = re.search(pattern, match, re.DOTALL | re.IGNORECASE)
            if abstract_match:
                abstract = abstract_match.group(1).strip()
                # æ¸…ç†HTMLæ ‡ç­¾ã€é“¾æ¥å’Œå¤šä½™ç©ºç™½
                abstract = re.sub(r'<a[^>]*>.*?</a>', '', abstract)  # ç§»é™¤More/Lessé“¾æ¥
                abstract = re.sub(r'<[^>]+>', '', abstract)
                abstract = re.sub(r'&hellip;.*', '', abstract)  # ç§»é™¤çœç•¥å·åŠåç»­å†…å®¹
                abstract = re.sub(r'\s+', ' ', abstract).strip()
                if len(abstract) > 20:  # ç¡®ä¿ä¸æ˜¯ç©ºçš„æˆ–å¤ªçŸ­çš„å†…å®¹
                    paper['abstract'] = abstract
                    break
        
        # æå–æäº¤æ—¥æœŸ
        submitted_pattern = r'<span[^>]+>Submitted</span>\s+([^;]+);'
        submitted_match = re.search(submitted_pattern, match)
        if submitted_match:
            paper['submitted_date'] = submitted_match.group(1).strip()
        
        # æå–è¯„è®ºä¿¡æ¯
        comments_pattern = r'<p class="comments[^"]*"[^>]*>.*?<span[^>]+>Comments:</span>\s*<span[^>]*>(.*?)</span>'
        comments_match = re.search(comments_pattern, match, re.DOTALL)
        if comments_match:
            comments = re.sub(r'<[^>]+>', '', comments_match.group(1)).strip()
            if comments:
                paper['comments'] = comments
        
        # åªæ·»åŠ è‡³å°‘æœ‰æ ‡é¢˜æˆ–arXiv IDçš„è®ºæ–‡
        if paper.get('title') or paper.get('arxiv_id'):
            papers.append(paper)
    
    return papers


def fetch_papers_for_researcher(author_name: str, date_from: str, date_to: str) -> List[Dict[str, Any]]:
    """
    è·å–ç‰¹å®šç ”ç©¶è€…åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„è®ºæ–‡
    
    Args:
        author_name: ç ”ç©¶è€…å§“å
        date_from: å¼€å§‹æ—¥æœŸ
        date_to: ç»“æŸæ—¥æœŸ
        
    Returns:
        è®ºæ–‡åˆ—è¡¨
    """
    try:
        # æ„å»ºæœç´¢URL
        search_url = build_arxiv_search_url(author_name, date_from, date_to)
        print(f"æœç´¢ {author_name} çš„è®ºæ–‡: {search_url}")
        
        # è·å–æœç´¢ç»“æœé¡µé¢
        with httpx.Client(follow_redirects=True, timeout=30.0) as client:
            response = client.get(search_url)
            response.raise_for_status()
        
        # è§£ææœç´¢ç»“æœ
        papers = parse_arxiv_search_results(response.text)
        
        # ä¸ºæ¯ç¯‡è®ºæ–‡æ·»åŠ æŸ¥è¯¢çš„ä½œè€…ä¿¡æ¯
        for paper in papers:
            paper['queried_author'] = author_name
            
        return papers
        
    except Exception as e:
        print(f"è·å– {author_name} çš„è®ºæ–‡æ—¶å‡ºé”™: {e}")
        return []


def get_today_papers_for_all_researchers(researchers: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    è·å–æ‰€æœ‰ç ”ç©¶è€…ä»Šå¤©å‘å¸ƒçš„è®ºæ–‡
    
    Args:
        researchers: ç ”ç©¶è€…åˆ—è¡¨
        
    Returns:
        æŒ‰ç ”ç©¶è€…åˆ†ç»„çš„è®ºæ–‡å­—å…¸
    """
    # è·å–ä»Šå¤©çš„æ—¥æœŸ
    today = datetime.now()
    date_str = today.strftime("%Y-%m-%d")
    
    print(f"\næ­£åœ¨æœç´¢ {date_str} å½“å¤©å‘å¸ƒçš„è®ºæ–‡...")
    print("=" * 60)
    
    all_papers = {}
    
    for researcher in researchers:
        # è·å–ç ”ç©¶è€…å§“å
        if isinstance(researcher, dict):
            if "name" in researcher:
                author_name = researcher["name"]
            else:
                # å–ç¬¬ä¸€ä¸ªéç©ºå€¼ä½œä¸ºå§“å
                author_name = next((v for v in researcher.values() if v.strip()), "")
        else:
            author_name = str(researcher)
        
        if not author_name or author_name.lower() in ['aaa', 'test']:  # è·³è¿‡æµ‹è¯•æ•°æ®
            continue
            
        print(f"\næ­£åœ¨æœç´¢ {author_name} çš„è®ºæ–‡...")
        
        # è·å–è¯¥ç ”ç©¶è€…çš„è®ºæ–‡
        papers = fetch_papers_for_researcher(author_name, date_str, date_str)
        
        if papers:
            all_papers[author_name] = papers
            print(f"  æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        else:
            print(f"  æœªæ‰¾åˆ°è®ºæ–‡")
    
    return all_papers


def display_papers(all_papers: Dict[str, List[Dict[str, Any]]]) -> None:
    """
    æ˜¾ç¤ºæ‰€æœ‰è®ºæ–‡
    
    Args:
        all_papers: æŒ‰ç ”ç©¶è€…åˆ†ç»„çš„è®ºæ–‡å­—å…¸
    """
    if not all_papers:
        print("\næ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ã€‚")
        return
    
    total_papers = sum(len(papers) for papers in all_papers.values())
    print(f"\nğŸ‰ å…±æ‰¾åˆ° {total_papers} ç¯‡è®ºæ–‡:")
    print("=" * 80)
    
    for author, papers in all_papers.items():
        print(f"\nğŸ‘¨â€ğŸ”¬ {author} ({len(papers)} ç¯‡è®ºæ–‡):")
        print("-" * 40)
        
        for i, paper in enumerate(papers, 1):
            print(f"\n{i}. ğŸ“„ {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
            
            if 'arxiv_id' in paper:
                print(f"   ğŸ”— arXiv ID: {paper['arxiv_id']}")
                print(f"   ğŸŒ é“¾æ¥: {paper.get('url', '')}")
            
            if 'authors' in paper and paper['authors']:
                authors_str = ", ".join(paper['authors'][:3])  # åªæ˜¾ç¤ºå‰3ä¸ªä½œè€…
                if len(paper['authors']) > 3:
                    authors_str += f" (ç­‰ {len(paper['authors'])} ä½ä½œè€…)"
                print(f"   ğŸ‘¥ ä½œè€…: {authors_str}")
            
            if 'submitted_date' in paper:
                print(f"   ğŸ“… æäº¤æ—¥æœŸ: {paper['submitted_date']}")
            
            if 'abstract' in paper and paper['abstract']:
                abstract = paper['abstract']
                print(f"   ğŸ“ æ‘˜è¦: {abstract}")


def display_researchers(researchers: List[Dict[str, Any]]) -> None:
    """
    æ˜¾ç¤ºç ”ç©¶è€…åˆ—è¡¨
    
    Args:
        researchers: ç ”ç©¶è€…æ•°æ®åˆ—è¡¨
    """
    if not researchers:
        print("æ²¡æœ‰æ‰¾åˆ°ç ”ç©¶è€…æ•°æ®")
        return
        
    print(f"\næ‰¾åˆ° {len(researchers)} ä¸ªç ”ç©¶è€…:")
    print("=" * 50)
    
    for i, researcher in enumerate(researchers, 1):
        print(f"{i}. ", end="")
        
        if isinstance(researcher, dict):
            if "name" in researcher:
                print(f"å§“å: {researcher['name']}")
                # æ˜¾ç¤ºå…¶ä»–å­—æ®µ
                for key, value in researcher.items():
                    if key != "name" and key != "row_index" and value:
                        print(f"   {key}: {value}")
            else:
                # æ˜¾ç¤ºæ‰€æœ‰å­—æ®µ
                for key, value in researcher.items():
                    if value:
                        print(f"{key}: {value}")
        else:
            print(researcher)
        print()


def get_recent_papers_for_researchers(researchers: List[Dict[str, Any]], days: int = 7) -> Dict[str, List[Dict[str, Any]]]:
    """
    è·å–æ‰€æœ‰ç ”ç©¶è€…æœ€è¿‘å‡ å¤©å‘å¸ƒçš„è®ºæ–‡
    
    Args:
        researchers: ç ”ç©¶è€…åˆ—è¡¨
        days: æœç´¢æœ€è¿‘å‡ å¤©
        
    Returns:
        æŒ‰ç ”ç©¶è€…åˆ†ç»„çš„è®ºæ–‡å­—å…¸
    """
    # è·å–æ—¥æœŸèŒƒå›´
    today = datetime.now()
    start_date = today - timedelta(days=days)
    end_date = today
    
    start_date_str = start_date.strftime("%Y-%m-%d")
    end_date_str = end_date.strftime("%Y-%m-%d")
    
    print(f"\næ­£åœ¨æœç´¢ {start_date_str} åˆ° {end_date_str} æœŸé—´å‘å¸ƒçš„è®ºæ–‡...")
    print("=" * 60)
    
    all_papers = {}
    
    for researcher in researchers:
        # è·å–ç ”ç©¶è€…å§“å
        if isinstance(researcher, dict):
            if "name" in researcher:
                author_name = researcher["name"]
            else:
                # å–ç¬¬ä¸€ä¸ªéç©ºå€¼ä½œä¸ºå§“å
                author_name = next((v for v in researcher.values() if v.strip()), "")
        else:
            author_name = str(researcher)
        
        if not author_name or author_name.lower() in ['aaa', 'test']:  # è·³è¿‡æµ‹è¯•æ•°æ®
            continue
            
        print(f"\næ­£åœ¨æœç´¢ {author_name} çš„è®ºæ–‡...")
        
        # è·å–è¯¥ç ”ç©¶è€…çš„è®ºæ–‡
        papers = fetch_papers_for_researcher(author_name, start_date_str, end_date_str)
        
        if papers:
            all_papers[author_name] = papers
            print(f"  æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        else:
            print(f"  æœªæ‰¾åˆ°è®ºæ–‡")
    
    return all_papers


def main():
    """ä¸»å‡½æ•°"""
    # Google Sheets TSV å¯¼å‡ºé“¾æ¥
    tsv_url = "https://docs.google.com/spreadsheets/d/1itjnV2U-Eh0F1T0LIGuLjzIhgL9f_OD8tbkMUG-Onic/export?format=tsv&id=1itjnV2U-Eh0F1T0LIGuLjzIhgL9f_OD8tbkMUG-Onic&gid=0"
    
    print("ğŸ” æ­£åœ¨ä» Google Sheets è·å–ç ”ç©¶è€…åˆ—è¡¨...")
    print(f"URL: {tsv_url}\n")
    
    # è·å–ç ”ç©¶è€…æ•°æ®
    researchers = fetch_researchers_from_tsv(tsv_url)
    
    # æ˜¾ç¤ºç ”ç©¶è€…åˆ—è¡¨
    display_researchers(researchers)
    
    if researchers:
        # è·å–æ‰€æœ‰ç ”ç©¶è€…ä»Šå¤©å‘å¸ƒçš„è®ºæ–‡
        all_papers = get_today_papers_for_all_researchers(researchers)
        
        # æ˜¾ç¤ºè®ºæ–‡ç»“æœ
        display_papers(all_papers)
        
        # å¦‚æœä»Šå¤©æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ï¼Œæœç´¢æœ€è¿‘ä¸€å‘¨çš„è®ºæ–‡
        if not all_papers:
            print("\nğŸ’¡ ä»Šå¤©æ²¡æœ‰æ–°è®ºæ–‡ï¼Œè®©æˆ‘ä»¬æœç´¢æœ€è¿‘ä¸€å‘¨çš„è®ºæ–‡...")
            recent_papers = get_recent_papers_for_researchers(researchers, days=7)
            if recent_papers:
                print(f"\nğŸ“š æœ€è¿‘ä¸€å‘¨çš„è®ºæ–‡ (æ³¨æ„ï¼šè¿™äº›ä¸æ˜¯ä»Šå¤©å‘å¸ƒçš„):")
                display_papers(recent_papers)
            else:
                print("\nğŸ“ æœ€è¿‘ä¸€å‘¨ä¹Ÿæ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ã€‚å¯èƒ½åŸå› ï¼š")
                print("   1. ç ”ç©¶è€…å§“åéœ€è¦æ ¸å®")
                print("   2. è¯¥ç ”ç©¶è€…æœ€è¿‘æ²¡æœ‰åœ¨ arXiv ä¸Šå‘å¸ƒè®ºæ–‡") 
                print("   3. å¯ä»¥å°è¯•æ›´é•¿çš„æ—¶é—´èŒƒå›´")
        
        return researchers, all_papers
    else:
        return [], {}


if __name__ == "__main__":
    researchers_data, papers_data = main()
